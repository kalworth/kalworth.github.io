# 李宏毅机器学习第三章任务

## 什么是梯度下降法？

### Review: 梯度下降法

在回归问题中，为了找到model的最合适的参数以此来找到最好的function，我们往往需要优化原有的function参数，也就是要不断的减小Loss函数。

为了解决这个问题，我们采用梯度下降法

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-1.png)

以多参数距离，分别对两个参数微分后使得函数向着减小的方向移动。

分别计算初始点处，两个参数对 L 的偏微分，然后 *θ**0 减掉 *η** 乘上偏微分的值，得到一组新的参数。同理反复进行这样的计算。黄色部分为简洁的写法，\triangledown L(\theta)▽**L**(**θ**)** 即为梯度。

### 优化梯度下降的方法

#### Tips 1：调整学习率（Learning Rate）

##### 选择合适的学习率

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-3.png)

上图左边黑色为损失函数的曲线，假设从左边最高点开始，如果学习率调整的刚刚好，比如红色的线，就能顺利找到最低点。如果学习率调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 学习率调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，更新参数的时候只会发现损失函数越更新越大。

因此在实际训练过程中选择合适的Learning Rate以及合适的epoch非常重要

##### 自适应学习率

举一个简单的思想：随着次数的增加，通过一些因子来减少学习率

* 通常刚开始，初始点会距离最低点比较远，所以使用大一点的学习率
* update好几次参数之后呢，比较靠近最低点了，此时减少学习率

学习率不能是一个值通用所有特征，不同的参数需要不同的学习率

##### Adagrad算法

对于经典的SGD优化方法，参数θ的更新为：

![](https://img-blog.csdn.net/20180621100038444?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

再来看AdaGrad算法表示为：

![](https://img-blog.csdn.net/20180621100112673?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数。

###### **dagrad算法分析**

 （1）从AdaGrad算法中可以看出，随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。

  （2）在SGD中，随着梯度
  的增大，我们的学习步长应该是增大的。但是在AdaGrad中，随着梯度g的增大，我们的r也在逐渐的增大，且在梯度更新时r在分母上，也就是整个学习率是减少的，这是为什么呢？

  这是因为随着更新次数的增大，我们希望学习率越来越慢。因为我们认为在学习率的最初阶段，我们距离损失函数最优解还很远，随着更新次数的增加，越来越接近最优解，所以学习率也随之变慢。

  （3）经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrade在某些深度学习模型上效果不错，但不是全部。

###### AdaGrad算法存在的矛盾

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-6.png)

在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-7.png)

最好的步伐应该是：不止和一次微分成正比，还和二次微分成反比。最好的step应该考虑到二次微分：

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-10.png)

###### Adagrad算法进一步的解释

再回到之前的 Adagrad

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-11.png)

这里的分母就是在代价尽量小的情况下模拟二次微分

#### Tips 2：随机梯度下降法

随机梯度下降法更快：

损失函数不需要处理训练集所有的数据，选取一个例子 x^n

此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数Ln，就可以赶紧update 梯度。

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-12.png)

常规梯度下降法走一步要处理到所有二十个例子，但随机算法此时已经走了二十步（每处理一个例子就更新）

#### Tips 3:  特征缩放

##### 为什么要进行特征缩放？

**1. 统一特征的权重&**提升模型准确性****

如果某个特征的取值范围比其他特征大很多，那么数值计算（比如说计算欧式距离）就受该特征的主要支配。但实际上并不一定是这个特征最重要，通常需要把每个特征看成同等重要。归一化/标准化数据可以使不同维度的特征放在一起进行比较，可以大大提高模型的准确性。

**2. 提升梯度下降法的收敛速度**

在使用梯度下降法求解最优化问题时， 归一化/标准化数据后可以加快梯度下降的求解速度。

##### 具体使用哪种方法进行特征缩放？

在需要使用距离来度量相似性的算法中，或者使用PCA技术进行降维的时候，通常使用 **标准化（s**tandardization**或**均值归一化（mean normalization）比较好，但如果数据分布不是正态分布或者标准差非常小，以及需要把数据固定在 [0, 1] 范围内，那么使用 **最大最小值归一化（min-max normalization）** 比较好（min-max 常用于归一化图像的灰度值）。但是min-max比较容易受异常值的影响，如果数据集包含较多的异常值，可以考虑使用 **稳键归一化（robust normalization）** 。对于已经中心化的数据或稀疏数据的缩放，比较推荐使用 **最大绝对值归一化（max abs normalization ）** ，因为它会保住数据中的０元素，不会破坏数据的稀疏性（sparsity）。

##### 哪些机器学习模型必须进行特征缩放？

通过梯度下降法求解的模型需要进行特征缩放，这包括线性回归（Linear Regression）、逻辑回归（Logistic Regression）、感知机（Perceptron）、支持向量机（SVM）、神经网络（Neural Network）等模型。此外，近邻法（KNN），K均值聚类（K-Means）等需要根据数据间的距离来划分数据的算法也需要进行特征缩放。主成分分析（PCA），线性判别分析（LDA）等需要计算特征的方差的算法也会受到特征缩放的影响。

决策树（Decision Tree），随机森林（Random Forest）等基于树的分类模型不需要进行特征缩放，因为特征缩放不会改变样本在特征上的信息增益。

##### 进行特征缩放的注意事项：

需要先把数据拆分成训练集与验证集，在训练集上计算出需要的数值（如均值和标准值），对训练集数据做标准化/归一化处理（不要在整个数据集上做标准化/归一化处理，因为这样会**将验证集的信息带入到训练集中，这是一个非常容易犯的错误），然后再用之前计算出的数据（如均值和标准值）对验证集数据做相同的标准化/归一化处理。**

## 梯度下降的理论基础

当用梯度下降解决问题：

每次更新参数 *θ*，都得到一个新的 *θ*，它都使得损失函数更小。即：

**L**(**θ**0)**>**L**(**θ**1**)**>**L**(**θ**2**)**>**⋅**⋅**⋅

上述结论正确吗？

结论是不正确的。。。

## 数学理论

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-16.png)

比如在 θ0 处，可以在一个小范围的圆圈内找到损失函数细小的 θ1，不断的这样去寻找。

接下来就是如果在小圆圈内快速的找到最小值？

### 泰勒展开式

先介绍一下泰勒展开式

定义

若 **h**(**x**) 在 x=x0 点的某个领域内有无限阶导数（即无限可微分，infinitely differentiable），那么在此领域内有：

![1657898557004](image/机器学习task04/1657898557004.png)

多变量泰勒展开即为对多变量求微分的相仿形式

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-18.png)

### 利用泰勒展开式简化

回到之前如何快速在圆圈内找到最小值。基于泰勒展开式，在 (a,b) 点的红色圆圈范围内，可以将损失函数用泰勒展开式进行简化：

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-19.png)

将问题进而简化为下图：

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-20.png)

不考虑s的话，可以看出剩下的部分就是两个向量(**△**θ**1,**△**θ**2) 和 **(**u**,**v**)** 的内积，那怎样让它最小，就是和向量 **(**u**,**v**)** 方向相反的向量。

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-21.png)

然后将u和v带入。

![](https://oss.linklearner.com/leeml/chapter6/res/chapter6-22.png)

发现最后的式子就是梯度下降的式子。但这里用这种方法找到这个式子有个前提，泰勒展开式给的损失函数的估算值是要足够精确的，而这需要红色的圈圈足够小（也就是学习率足够小）来保证。所以理论上每次更新参数都想要损失函数减小的话，即保证式1-2 成立的话，就需要学习率足够足够小才可以。

所以实际中，当更新参数的时候，如果学习率没有设好，是有可能式1-2是不成立的，所以导致做梯度下降的时候，损失函数没有越来越小。

式1-2只考虑了泰勒展开式的一次项，如果考虑到二次项（比如牛顿法），在实际中不是特别好，会涉及到二次微分等，多很多的运算，性价比不好。

## 梯度下降的限制

这些内容在前面已经阐述过，此处不再赘述。
