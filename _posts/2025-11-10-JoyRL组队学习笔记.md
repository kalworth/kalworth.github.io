# JoyRL  组队学习学习笔记
首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：https://github.com/datawhalechina/joyrl-book
https://github.com/datawhalechina/easy-rl

## 一、马尔可夫决策过程
![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch2/figs/interaction_mdp.png)

### 1.基础概念

如上图所示，马尔可夫决策过程描述的是智能体与环境交互的过程，在当前时刻$t$，状态$S_t$是对环境的一种描述，采取能够对环境造成影响的动作$A_t$，使得环境状态由$S_t \rightarrow S_{t+1}$，并且环境给予智能体一个动作$A_t$带来的反馈(即奖励$R_t$)。

![](https://img-blog.csdnimg.cn/b64b9d24357c472c9fca4dc6f172c1cd.gif#pic_center)

以gym中pendulum单摆环境为例，

状态$S_t$表示为一组描述单摆当前状态的向量，即$S_t=\begin{bmatrix} cos \theta &sin \theta & \dot{\theta} \end{bmatrix}_t$，其中$\theta$代表单摆竖直方向夹角。

动作$A_t=T_t$，其中$T_t$是向单摆施加的扭矩Torque，取值Torque $\in$[-2,2]。

单摆的平衡状态$S_{balance}=\begin{bmatrix} 1 &0 & 0 \end{bmatrix}$。

采取动作$A_t$后，单摆环境根据微分方程更新进入下一时刻的环境状态$S_{t+1}$。

PS：这里的微分方程是对系统进行力学分析or其他方法得到的运动微分方程，运动方程描述了系统运动的规律。

并且设置奖励函数(reward function):$R_t=\theta_t^2 + 0.1 \dot{\theta}_t^2 + 0.001 * (T^2)$作为反馈。