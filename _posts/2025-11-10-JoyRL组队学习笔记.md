# JoyRL组队学习笔记

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

笔记涉及图片来源于网络，如有侵权，请联系删除。

## 一、马尔可夫决策过程
![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch2/figs/interaction_mdp.png)

### 1.基础概念

如上图所示，马尔可夫决策过程描述的是智能体与环境交互的过程，在当前时刻$t$，状态$s_t$是对环境的一种描述，采取能够对环境造成影响的动作$a_t$，使得环境状态由$s_t \rightarrow s_{t+1}$，并且环境给予智能体反馈(即奖励$r_t$)。

![](https://img.remit.ee/api/file/BQACAgUAAyEGAASHRsPbAAEGXUJpEtuSM7g2z__3lxbFfhOkgztHtAACfFwAAlOumFRTxoW8T0G0DTYE.png)

以gym中pendulum单摆环境为例，

状态$S_t$表示为一组描述单摆当前状态的向量，即$s_t=\begin{bmatrix} cos \theta &sin \theta & \dot{\theta} \end{bmatrix}_t$，其中$\theta$代表单摆竖直方向夹角。

动作$a_t=T_t$，其中$T_t$是向单摆施加的扭矩Torque，取值Torque $\in$[-2,2]。

单摆的平衡状态$s_{balance}=\begin{bmatrix} 1 &0 & 0 \end{bmatrix}$。

采取动作$a_t$后，单摆环境根据微分方程更新进入下一时刻的环境状态$s_{t+1}$。

PS：这里的微分方程是对系统进行力学分析or其他方法得到的运动微分方程，运动方程描述了系统运动的规律。

并且设置奖励函数(reward function)：$r_t=\theta_t^2 + 0.1 \dot{\theta}_t^2 + 0.001 * (T^2)$作为反馈。

在每个时刻t，有$s_t,a_t,r_t$完整描述这一时刻的交互过程，这样的过程从t=0时刻开始不断进行，我们就可以得到一条轨迹(Trajectory)，或者说历史序列：

$$
{s_0,a_0,r_0},{s_1,a_1,r_1},{s_2,a_2,r_3},\cdots,{s_T,a_T,r_T}
$$

这一序列由初始t=0时刻一直进行到终止t=T时刻，即一个回合(episode)。通常T是一个有限步长。

在马尔可夫决策过程当中，除了上述

状态空间$S$：所有环境状态$s_t$的集合，$s_t \in S$

动作空间$A$：智能体可能采取的所有动作，$a_t \in A$

奖励函数$R$：定义即时获取的奖励

此外还有

状态转移概率$P$：由当前状态和动作下，环境进入到下一个状态的概率分布。

折扣因子$\gamma$：取值在0到1之间，大小取决于即时奖励和未来奖励的重要程度。

综上，可以利用一个五元组来表示马尔可夫决策过程(MDP)：

$$
MDP=(S,A,R,P,\gamma)
$$

### 2.马尔可夫性质与状态转移矩阵

马尔可夫决策过程是基于马尔可夫性质进行的。马尔可夫性质：系统未来状态的概率分布只依赖于当前的状态和动作，而与过去的状态和动作无关：
$$
P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\cdots,s_0,a_0)=P(s_{t+1}|s_t,a_t)
$$

需要注意的一点是，真实环境中，满足马尔可夫性质的情况不多见。但是可以通过近似将一个不太依赖于过往状态，即只依赖于当前状态就能预测下一个状态的过程看作满足马尔可夫性质。

保留状态空间和状态转移概率，可以看作马尔科夫链，如下图所示：

![](https://youke1.picui.cn/s1/2025/11/11/6912ea3152eb0.png)

在状态$s_1$时，有0.2的概率进入$s_1$，0.4的概率进入$s_2$，0.4的概率就进入$s_3$。定义状态转移矩阵
$$
P_{ss'}=P(S_{t+1}=s'|S_t=s)=
\begin{bmatrix} 
p_{11}&p_{12}&\cdots&p_{1n}\\
p_{21}&p_{22}&\cdots&p_{2n}\\
p_{31}&p_{32}&\cdots&p_{3n}\\
p_{41}&p_{42}&\cdots&p_{4n}\\
\end{bmatrix}
$$

其中，第行$i$表示当前处于状态$s_i$,第$j$列表示进入下一个状态$s_j$。

$p_{ij}$表示在当前状态$s_i$下，进入到下一个状态$s_j$的概率是多少，且$\sum^n_{j=1}p_{ij}=1,i=1,2,3,\cdots$

思考:这种状态转移矩阵描述只用于离散状态空间。

### 3.目标与回报

强化学习的目标是最大化累积奖励$G_t$

$$
\begin{aligned}
G_t&=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+2}+\cdots\\
&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+2}+\cdots)\\
&=R_{t+1}+\gamma G_{t+1}\\
\end{aligned}
$$

这里同样体现了折扣因子$\gamma$是衡量即时奖励和未来奖励重要程度的参数。

### 4.价值

#### 4.1 状态价值

状态价值函数$V_\pi(s)$是在给定策略$\pi$的情况下，只由当前状态决定的期望累积奖励

$$
V_\pi(s)=E_{\pi}[G_t|S_t=s]
$$

更进一步的，$E_{\pi}$代表在策略$\pi$下的期望值，那么$V_\pi(s)$是策略$\pi$下，给定状态$s_t$的平均加权累积奖励。一般情况下，策略$\pi$下采取的动作不是固定的动作序列，而是多个动作序列构成的集合$(a_1,a_2,a_3,\cdots,a_n)$，比如采取的动作序列的概率是$p_i$，采取的动作序列是$a_i=(a_{11},a_{12},a_{13},\cdots,a_{1T})$，这套序列带来的累积奖励是$G_i$，那么状态价值函数$V_\pi(s)$为：

$$
\sum^n_{i=1}p_iG_i
$$

#### 4.2 动作价值

动作价值函数$Q_\pi(s,a)$描述了在给定动作和给定状态的情况下智能体得到的期望累计奖励

$$
Q_\pi(s,a)=E_{\pi}[G_t|S_t=s,A_t=a])
$$

动作价值函数与状态价值函数的联系可以表示为：
$$
V_\pi(s)=\sum_{a \in A}\pi(a|s)Q_\pi(s,a)
$$

即，当动作$a$按照策略$\pi$取动作空间$A$种所有可能情况下，动作价值函数$Q_\pi(s,a)$的加权总和与状态价值函数$V_\pi(s)$等价。

### 5.有模型、无模型、预测与控制

有模型(Model-based)：对环境和奖励函数进行建模，不与环境直接交互情况下，预测奖励与状态，并且制定对应策略。可以理解为仿真or赛博交互，包括动态规划(DynamicProgramming)等方法。

无模型(Model-free)：不依赖环境模型，或者说环境模型难以建立的情况下，将智能体与环境进行交互的过程可以看作一个黑盒，通过试错的方法去学习状态价值函数或者动作价值函数。简单来说，即通过交互试错，让智能体知道什么样的做法是好的。这种方法适用于复杂or未知的环境，包括Q-Learning、SARSA等算法。

预测：给定策略$\pi$，计算$V_\pi(s)$的值 or 给定动作和状态$s,a$，计算$Q_\pi(s,a)$的值，来评价策略的好坏(给定的$a$也由策略决定)。

控制：自动化专业的同学肯定很熟悉Control了，按照控制专业的思路来说，控制就是不管用什么方法(PID、LQR、MPC、$\cdots$)找到一个控制量u，让系统输出y能够达到自己期望。强化学习中的control，同理，是为了找到一个最好的策略$\pi^*$,按照这个策略去执行让期望累积奖励最大的动作。用于控制任务的算法主要包括动态规划(Dynamic Programming)、Q学习(Q-Learning)、策略梯度方法(Policy Gradient)等。

预测与控制的联系：预测相当于一个Sensor，通过预测我们可以知道价值，利用价值去学习策略(即控制)。

## 二、动态规划

### 1.基础概念

动态规划(Dynamic Programming)是一种解决复杂问题的数学方法，主要通过将问题分成子问题，并提供子问题的解决方案，从而避免重复计算，提高解决问题的效率。以求取路径之和问题为例，其具有以下三个性质：

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch3/figs/robot_maze.png)

- 重叠子问题：指子问题在解决问题过程中会被多次计算。计算机器人从起点到终点的路径之和，某个到达当前位置的路径可以由且只由上一个位置的路径决定————子问题。在机器人探索过程中，这个子问题每到达一个新位置，都会被计算出一次，即子问题被多次计算，具有重叠子问题的性质。

- 最优化原理：问题的解决可以由子问题的最优解来构建。机器人每次只能往右或者往下运动一格，那么以$f(i,j)$来表示到达当前位置$(i,j)$的路径之和，那么当前位置的路径之和只由左边一格的路径之和和上边一格的路径之和决定，构建出子问题的最优解，即状态转移方程
$$
f(i,j)=f(i-1,j)+f(i,j-1)
$$

- 无后效性：未来状态只依赖于当前状态，与过去的状态无关。从求取路径之和问题以及其状态转移方程来看，未来一格的路径之和只与当前可能的一格的路径之和有关，即满足无后效性，无后效性满足马尔可夫性质。

### 2.贝尔曼方程

动态规划方法在强化学习中，常被用来求取价值函数以及最优策略，其中核心概念是贝尔曼方程。对状态价值函数以及动作价值函数进行分解，分解成当前状态和未来状态的形式(当前+未来)，我们就可以得到贝尔曼方程。

贝尔曼方程的状态价值形式

$$
\begin{aligned}
V_\pi(s)&=E_{\pi}[G_t|S_t=s]\\
&=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
\end{aligned}
$$

拆解为当前状态和下一时刻状态的形式，$\gamma$是设定的参数，可以提到外边

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=E_{\pi}[R_{t+1}|S_t=s]+\gamma E_{\pi}[G_{t+1}|S_t=s']\\
&=E_{\pi}[R_{t+1}|S_t=s]+\gamma V_\pi(S_{t+1})
\end{aligned}
$$

$\gamma V_\pi(S_{t+1})$是下一时刻的状态价值，状态价值可写为

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=E_{\pi}[R_{t+1}+\gamma V_\pi(S_{t+1})|S_t=s]
\end{aligned}
$$

引入状态转移概率$P(s'|s,a)$
表示在当前状态$s$下采取动作$a$，
进入到下一个状态$s'$的概率分布，
以及策略$\pi(a|s)$
表示在$s$状态下，
采取动作$a$的概率分布(可以是确定性策略或者随机性策略,确定性指在同一状态下采取相同的策略，随机性策略指在某个状态下根据策略的概率分布选择策略)。状态价值形式的贝尔曼方程可以写为

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=\sum_{a \in A}\pi(a|s)\sum_{s' \in S}P(s'|s,a)[r+\gamma V_\pi(s')]
\end{aligned}
$$
其中，
$$
\sum_{s' \in S}P(s'|s,a)r=R(s,a)
$$

这一形式表现为"当前+未来"的递归形式，与前文动态规划中的求取路径之和的状态转移方程类似。贝尔曼方程在强化学习当中扮演的角色，就是动态规划中的状态转移方程。

相同的，贝尔曼方程的动作价值形式(Q函数)可以写为

$$
Q_\pi(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a) \sum_{a' \in A}\pi(a'|s')Q_\pi(s',a')
$$

### 3.策略迭代与价值迭代

策略迭代是为了找到一个策略$\pi^*$，在任意状态$s$下，得到的回报最大。策略迭代包含两个过程，

- 策略估计过程：在策略$\pi$下，求取价值$V^*$。按照策略$\pi$，计算在每个状态下的价值$V$,当$V$收敛时，停止循环。

![](https://youke1.picui.cn/s1/2025/11/12/69144dfbb3139.png)

- 策略改进过程：利用策略$\pi$生成动作$a_{temp}$，采取贪心策略(greedy)更新策略(概率$\epsilon$取最大化动作价值函数Q的动作 or 概率$(1-\epsilon)$选取随机一个动作)，当策略$\pi$收敛时，停止循环。

![](https://youke1.picui.cn/s1/2025/11/12/69144fde56f79.png)

策略迭代的结果就是策略$\pi$和价值函数$V$会收敛到最优$\pi^*$,
$V^*$

价值迭代更为简单粗暴，直接所有的动作，找到令回报最大的价值函数

$$
V(s) \leftarrow \max_{a \in A}(R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V(s'))
$$

在策略选取方面，使用确定性策略，直接选取令回报最大的动作$a$，
即$\pi(s)=arg\max_{a}\sum_{s',r}P(s',r|s,a)[r+\gamma V(s')]$

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch3/figs/vi_pseu.png)

策略迭代与价值迭代的区别：

策略迭代迭代两个目标，价值函数和策略一起收敛，参与动作价值计算的动作$a$随着策略的迭代而变化，即收敛过程是跳变的。而价值迭代要遍历计算所有的动作下的价值，从迭代速度来说，策略迭代更胜一筹。

