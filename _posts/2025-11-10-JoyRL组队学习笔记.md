# JoyRL组队学习笔记

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

笔记涉及图片来源于网络，如有侵权，请联系删除。

## 一、马尔可夫决策过程
![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch2/figs/interaction_mdp.png)

### 1.基础概念
__
如上图所示，马尔可夫决策过程描述的是智能体与环境交互的过程，在当前时刻$t$，状态$s_t$是对环境的一种描述，采取能够对环境造成影响的动作$a_t$，使得环境状态由$s_t \rightarrow s_{t+1}$，并且环境给予智能体反馈(即奖励$r_t$)。

![](https://img.remit.ee/api/file/BQACAgUAAyEGAASHRsPbAAEGXUJpEtuSM7g2z__3lxbFfhOkgztHtAACfFwAAlOumFRTxoW8T0G0DTYE.png)

以gym中pendulum单摆环境为例，

状态$S_t$表示为一组描述单摆当前状态的向量，即$s_t=\begin{bmatrix} cos \theta &sin \theta & \dot{\theta} \end{bmatrix}_t$，其中$\theta$代表单摆竖直方向夹角。

动作$a_t=T_t$，其中$T_t$是向单摆施加的扭矩Torque，取值Torque $\in$[-2,2]。

单摆的平衡状态$s_{balance}=\begin{bmatrix} 1 &0 & 0 \end{bmatrix}$。

采取动作$a_t$后，单摆环境根据微分方程更新进入下一时刻的环境状态$s_{t+1}$。

PS：这里的微分方程是对系统进行力学分析or其他方法得到的运动微分方程，运动方程描述了系统运动的规律。

并且设置奖励函数(reward function)：$r_t=\theta_t^2 + 0.1 \dot{\theta}_t^2 + 0.001 * (T^2)$作为反馈。

在每个时刻t，有$s_t,a_t,r_t$完整描述这一时刻的交互过程，这样的过程从t=0时刻开始不断进行，我们就可以得到一条轨迹(Trajectory)，或者说历史序列：

$$
{s_0,a_0,r_0},{s_1,a_1,r_1},{s_2,a_2,r_3},\cdots,{s_T,a_T,r_T}
$$

这一序列由初始t=0时刻一直进行到终止t=T时刻，即一个回合(episode)。通常T是一个有限步长。

在马尔可夫决策过程当中，除了上述

状态空间$S$：所有环境状态$s_t$的集合，$s_t \in S$

动作空间$A$：智能体可能采取的所有动作，$a_t \in A$

奖励函数$R$：定义即时获取的奖励

此外还有

状态转移概率$P$：由当前状态和动作下，环境进入到下一个状态的概率分布。

折扣因子$\gamma$：取值在0到1之间，大小取决于即时奖励和未来奖励的重要程度。

综上，可以利用一个五元组来表示马尔可夫决策过程(MDP)：

$$
MDP=(S,A,R,P,\gamma)
$$

### 2.马尔可夫性质与状态转移矩阵

马尔可夫决策过程是基于马尔可夫性质进行的。马尔可夫性质：系统未来状态的概率分布只依赖于当前的状态和动作，而与过去的状态和动作无关：
$$
P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\cdots,s_0,a_0)=P(s_{t+1}|s_t,a_t)
$$

需要注意的一点是，真实环境中，满足马尔可夫性质的情况不多见。但是可以通过近似将一个不太依赖于过往状态，即只依赖于当前状态就能预测下一个状态的过程看作满足马尔可夫性质。

保留状态空间和状态转移概率，可以看作马尔科夫链，如下图所示：

![](https://youke1.picui.cn/s1/2025/11/11/6912ea3152eb0.png)

在状态$s_1$时，有0.2的概率进入$s_1$，0.4的概率进入$s_2$，0.4的概率就进入$s_3$。定义状态转移矩阵
$$
P_{ss'}=P(S_{t+1}=s'|S_t=s)=
\begin{bmatrix} 
p_{11}&p_{12}&\cdots&p_{1n}\\
p_{21}&p_{22}&\cdots&p_{2n}\\
p_{31}&p_{32}&\cdots&p_{3n}\\
p_{41}&p_{42}&\cdots&p_{4n}\\
\end{bmatrix}
$$

其中，第行$i$表示当前处于状态$s_i$,第$j$列表示进入下一个状态$s_j$。

$p_{ij}$表示在当前状态$s_i$下，进入到下一个状态$s_j$的概率是多少，且$\sum^n_{j=1}p_{ij}=1,i=1,2,3,\cdots$

思考:这种状态转移矩阵描述只用于离散状态空间。

### 3.目标与回报

强化学习的目标是最大化累积奖励$G_t$

$$
\begin{aligned}
G_t&=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+2}+\cdots\\
&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+2}+\cdots)\\
&=R_{t+1}+\gamma G_{t+1}\\
\end{aligned}
$$

这里同样体现了折扣因子$\gamma$是衡量即时奖励和未来奖励重要程度的参数。

### 4.价值

#### 4.1 状态价值

状态价值函数$V_\pi(s)$是在给定策略$\pi$的情况下，只由当前状态决定的期望累积奖励

$$
V_\pi(s)=E_{\pi}[G_t|S_t=s]
$$

更进一步的，$E_{\pi}$代表在策略$\pi$下的期望值，那么$V_\pi(s)$是策略$\pi$下，给定状态$s_t$的平均加权累积奖励。一般情况下，策略$\pi$下采取的动作不是固定的动作序列，而是多个动作序列构成的集合($\mathbf{a}_1,\mathbf{a}_2,\mathbf{a}_3,\cdots,\mathbf{a}_n$)，比如采取的动作序列的概率是$p_i$，采取的动作序列是$\mathbf{a}_i=(a_{11},a_{12},a_{13},\cdots,a_{1T})$，这套序列带来的累积奖励是$G_i$，那么状态价值函数$V_\pi(s)$为：

$$
\sum^n_{i=1}p_iG_i
$$

#### 4.2 动作价值

动作价值函数$Q_\pi(s,a)$描述了在给定动作和给定状态的情况下智能体得到的期望累计奖励

$$
Q_\pi(s,a)=E_{\pi}[G_t|S_t=s,A_t=a])
$$

动作价值函数与状态价值函数的联系可以表示为：
$$
V_\pi(s)=\sum_{a \in A}\pi(a|s)Q_\pi(s,a)
$$

即，当动作$a$按照策略$\pi$取动作空间$A$种所有可能情况下，动作价值函数$Q_\pi(s,a)$的加权总和与状态价值函数$V_\pi(s)$等价。

### 5.有模型、无模型、预测与控制

有模型(Model-based)：对环境和奖励函数进行建模，不与环境直接交互情况下，预测奖励与状态，并且制定对应策略。可以理解为仿真or赛博交互，包括动态规划(DynamicProgramming)等方法。

无模型(Model-free)：不依赖环境模型，或者说环境模型难以建立的情况下，将智能体与环境进行交互的过程可以看作一个黑盒，通过试错的方法去学习状态价值函数或者动作价值函数。简单来说，即通过交互试错，让智能体知道什么样的做法是好的。这种方法适用于复杂or未知的环境，包括Q-Learning、SARSA等算法。

预测：给定策略$\pi$，计算$V_\pi(s)$的值 or 给定动作和状态$s,a$，计算$Q_\pi(s,a)$的值，来评价策略的好坏(给定的$a$也由策略决定)。

控制：自动化专业的同学肯定很熟悉Control了，按照控制专业的思路来说，控制就是不管用什么方法(PID、LQR、MPC、$\cdots$)找到一个控制量u，让系统输出y能够达到自己期望。强化学习中的control，同理，是为了找到一个最好的策略$\pi^*$,按照这个策略去执行让期望累积奖励最大的动作。用于控制任务的算法主要包括动态规划(DynamicProgramming
)、Q学习(Q-Learning)、策略梯度方法(Policy Gradient)等。