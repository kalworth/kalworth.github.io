# JoyRL组队学习笔记

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

笔记涉及图片来源于网络，如有侵权，请联系删除。

## 一、马尔可夫决策过程
![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch2/figs/interaction_mdp.png)

### 1.基础概念

如上图所示，马尔可夫决策过程描述的是智能体与环境交互的过程，在当前时刻$t$，状态$s_t$是对环境的一种描述，采取能够对环境造成影响的动作$a_t$，使得环境状态由$s_t \rightarrow s_{t+1}$，并且环境给予智能体反馈(即奖励$r_t$)。

![](https://img.remit.ee/api/file/BQACAgUAAyEGAASHRsPbAAEGXUJpEtuSM7g2z__3lxbFfhOkgztHtAACfFwAAlOumFRTxoW8T0G0DTYE.png)

以gym中pendulum单摆环境为例，

状态$S_t$表示为一组描述单摆当前状态的向量，即$s_t=\begin{bmatrix} cos \theta &sin \theta & \dot{\theta} \end{bmatrix}_t$，其中$\theta$代表单摆竖直方向夹角。

动作$a_t=T_t$，其中$T_t$是向单摆施加的扭矩Torque，取值Torque $\in$[-2,2]。

单摆的平衡状态$s_{balance}=\begin{bmatrix} 1 &0 & 0 \end{bmatrix}$。

采取动作$a_t$后，单摆环境根据微分方程更新进入下一时刻的环境状态$s_{t+1}$。

PS：这里的微分方程是对系统进行力学分析or其他方法得到的运动微分方程，运动方程描述了系统运动的规律。

并且设置奖励函数(reward function)：$r_t=\theta_t^2 + 0.1 \dot{\theta}_t^2 + 0.001 * (T^2)$作为反馈。

在每个时刻t，有$s_t,a_t,r_t$完整描述这一时刻的交互过程，这样的过程从t=0时刻开始不断进行，我们就可以得到一条轨迹(Trajectory)，或者说历史序列：

$$
{s_0,a_0,r_0},{s_1,a_1,r_1},{s_2,a_2,r_3},\cdots,{s_T,a_T,r_T}
$$

这一序列由初始t=0时刻一直进行到终止t=T时刻，即一个回合(episode)。通常T是一个有限步长。

在马尔可夫决策过程当中，除了上述

状态空间$S$：所有环境状态$s_t$的集合，$s_t \in S$

动作空间$A$：智能体可能采取的所有动作，$a_t \in A$

奖励函数$R$：定义即时获取的奖励

此外还有

状态转移概率$P$：由当前状态和动作下，环境进入到下一个状态的概率分布。

折扣因子$\gamma$：取值在0到1之间，大小取决于即时奖励和未来奖励的重要程度。

综上，可以利用一个五元组来表示马尔可夫决策过程(MDP)：

$$
MDP=(S,A,R,P,\gamma)
$$

### 2.马尔可夫性质与状态转移矩阵

马尔可夫决策过程是基于马尔可夫性质进行的。马尔可夫性质：系统未来状态的概率分布只依赖于当前的状态和动作，而与过去的状态和动作无关：
$$
P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\cdots,s_0,a_0)=P(s_{t+1}|s_t,a_t)
$$

需要注意的一点是，真实环境中，满足马尔可夫性质的情况不多见。但是可以通过近似将一个不太依赖于过往状态，即只依赖于当前状态就能预测下一个状态的过程看作满足马尔可夫性质。

保留状态空间和状态转移概率，可以看作马尔科夫链，如下图所示：

![](https://youke1.picui.cn/s1/2025/11/11/6912ea3152eb0.png)

在状态$s_1$时，有0.2的概率进入$s_1$，0.4的概率进入$s_2$，0.4的概率就进入$s_3$。定义状态转移矩阵
$$
P_{ss'}=P(S_{t+1}=s'|S_t=s)=
\begin{bmatrix} 
p_{11}&p_{12}&\cdots&p_{1n}\\
p_{21}&p_{22}&\cdots&p_{2n}\\
p_{31}&p_{32}&\cdots&p_{3n}\\
p_{41}&p_{42}&\cdots&p_{4n}\\
\end{bmatrix}
$$

其中，第行$i$表示当前处于状态$s_i$,第$j$列表示进入下一个状态$s_j$。

$p_{ij}$表示在当前状态$s_i$下，进入到下一个状态$s_j$的概率是多少，且$\sum^n_{j=1}p_{ij}=1,i=1,2,3,\cdots$

思考:这种状态转移矩阵描述只用于离散状态空间。

### 3.目标与回报

强化学习的目标是最大化累积奖励$G_t$

$$
\begin{aligned}
G_t&=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+2}+\cdots\\
&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+2}+\cdots)\\
&=R_{t+1}+\gamma G_{t+1}\\
\end{aligned}
$$

这里同样体现了折扣因子$\gamma$是衡量即时奖励和未来奖励重要程度的参数。

### 4.价值

#### 4.1 状态价值

状态价值函数$V_\pi(s)$是在给定策略$\pi$的情况下，只由当前状态决定的期望累积奖励

$$
V_\pi(s)=E_{\pi}[G_t|S_t=s]
$$

更进一步的，$E_{\pi}$代表在策略$\pi$下的期望值，那么$V_\pi(s)$是策略$\pi$下，给定状态$s_t$的平均加权累积奖励。一般情况下，策略$\pi$下采取的动作不是固定的动作序列，而是多个动作序列构成的集合$(a_1,a_2,a_3,\cdots,a_n)$，比如采取的动作序列的概率是$p_i$，采取的动作序列是$a_i=(a_{11},a_{12},a_{13},\cdots,a_{1T})$，这套序列带来的累积奖励是$G_i$，那么状态价值函数$V_\pi(s)$为：

$$
\sum^n_{i=1}p_iG_i
$$

#### 4.2 动作价值

动作价值函数$Q_\pi(s,a)$描述了在给定动作和给定状态的情况下智能体得到的期望累计奖励

$$
Q_\pi(s,a)=E_{\pi}[G_t|S_t=s,A_t=a])
$$

动作价值函数与状态价值函数的联系可以表示为：
$$
V_\pi(s)=\sum_{a \in A}\pi(a|s)Q_\pi(s,a)
$$

即，当动作$a$按照策略$\pi$取动作空间$A$种所有可能情况下，动作价值函数$Q_\pi(s,a)$的加权总和与状态价值函数$V_\pi(s)$等价。

### 5.有模型、无模型、预测与控制

有模型(Model-based)：对环境和奖励函数进行建模，不与环境直接交互情况下，预测奖励与状态，并且制定对应策略。可以理解为仿真or赛博交互，包括动态规划(DynamicProgramming)等方法。

无模型(Model-free)：不依赖环境模型，或者说环境模型难以建立的情况下，将智能体与环境进行交互的过程可以看作一个黑盒，通过试错的方法去学习状态价值函数或者动作价值函数。简单来说，即通过交互试错，让智能体知道什么样的做法是好的。这种方法适用于复杂or未知的环境，包括Q-Learning、SARSA等算法。

预测：给定策略$\pi$，计算$V_\pi(s)$的值 or 给定动作和状态$s,a$，计算$Q_\pi(s,a)$的值，来评价策略的好坏(给定的$a$也由策略决定)。

控制：自动化专业的同学肯定很熟悉Control了，按照控制专业的思路来说，控制就是不管用什么方法(PID、LQR、MPC、$\cdots$)找到一个控制量u，让系统输出y能够达到自己期望。强化学习中的control，同理，是为了找到一个最好的策略$\pi^*$,按照这个策略去执行让期望累积奖励最大的动作。用于控制任务的算法主要包括动态规划(Dynamic Programming)、Q学习(Q-Learning)、策略梯度方法(Policy Gradient)等。

预测与控制的联系：预测相当于一个Sensor，通过预测我们可以知道价值，利用价值去学习策略(即控制)。

## 二、动态规划

### 1.基础概念

动态规划(Dynamic Programming)是一种解决复杂问题的数学方法，主要通过将问题分成子问题，并提供子问题的解决方案，从而避免重复计算，提高解决问题的效率。以求取路径之和问题为例，其具有以下三个性质：

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch3/figs/robot_maze.png)

- 重叠子问题：指子问题在解决问题过程中会被多次计算。计算机器人从起点到终点的路径之和，某个到达当前位置的路径可以由且只由上一个位置的路径决定————子问题。在机器人探索过程中，这个子问题每到达一个新位置，都会被计算出一次，即子问题被多次计算，具有重叠子问题的性质。

- 最优化原理：问题的解决可以由子问题的最优解来构建。机器人每次只能往右或者往下运动一格，那么以$f(i,j)$来表示到达当前位置$(i,j)$的路径之和，那么当前位置的路径之和只由左边一格的路径之和和上边一格的路径之和决定，构建出子问题的最优解，即状态转移方程
$$
f(i,j)=f(i-1,j)+f(i,j-1)
$$

- 无后效性：未来状态只依赖于当前状态，与过去的状态无关。从求取路径之和问题以及其状态转移方程来看，未来一格的路径之和只与当前可能的一格的路径之和有关，即满足无后效性，无后效性满足马尔可夫性质。

### 2.贝尔曼方程

动态规划方法在强化学习中，常被用来求取价值函数以及最优策略，其中核心概念是贝尔曼方程。对状态价值函数以及动作价值函数进行分解，分解成当前状态和未来状态的形式(当前+未来)，我们就可以得到贝尔曼方程。

贝尔曼方程的状态价值形式

$$
\begin{aligned}
V_\pi(s)&=E_{\pi}[G_t|S_t=s]\\
&=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
\end{aligned}
$$

拆解为当前状态和下一时刻状态的形式，$\gamma$是设定的参数，可以提到外边

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=E_{\pi}[R_{t+1}|S_t=s]+\gamma E_{\pi}[G_{t+1}|S_t=s']\\
&=E_{\pi}[R_{t+1}|S_t=s]+\gamma V_\pi(S_{t+1})
\end{aligned}
$$

$\gamma V_\pi(S_{t+1})$是下一时刻的状态价值，状态价值可写为

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=E_{\pi}[R_{t+1}+\gamma V_\pi(S_{t+1})|S_t=s]
\end{aligned}
$$

引入状态转移概率$P(s'|s,a)$
表示在当前状态$s$下采取动作$a$，
进入到下一个状态$s'$的概率分布，
以及策略$\pi(a|s)$
表示在$s$状态下，
采取动作$a$的概率分布(可以是确定性策略或者随机性策略,确定性指在同一状态下采取相同的策略，随机性策略指在某个状态下根据策略的概率分布选择策略)。状态价值形式的贝尔曼方程可以写为

$$
\begin{aligned}
V_\pi(s)&=\cdots\\
&=\sum_{a \in A}\pi(a|s)\sum_{s' \in S}P(s'|s,a)[r+\gamma V_\pi(s')]
\end{aligned}
$$
其中，
$$
\sum_{s' \in S}P(s'|s,a)r=R(s,a)
$$

这一形式表现为"当前+未来"的递归形式，与前文动态规划中的求取路径之和的状态转移方程类似。贝尔曼方程在强化学习当中扮演的角色，就是动态规划中的状态转移方程。

相同的，贝尔曼方程的动作价值形式(Q函数)可以写为

$$
Q_\pi(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a) \sum_{a' \in A}\pi(a'|s')Q_\pi(s',a')
$$

### 3.策略迭代与价值迭代

策略迭代是为了找到一个策略$\pi^*$，在任意状态$s$下，得到的回报最大。策略迭代包含两个过程，

- 策略估计过程：在策略$\pi$下，求取价值$V^*$。按照策略$\pi$，计算在每个状态下的价值$V$,当$V$收敛时，停止循环。

![](https://youke1.picui.cn/s1/2025/11/12/69144dfbb3139.png)

- 策略改进过程：利用策略$\pi$生成动作$a_{temp}$，采取贪心策略(greedy)更新策略(概率$\epsilon$取最大化动作价值函数Q的动作 or 概率$(1-\epsilon)$选取随机一个动作)，当策略$\pi$收敛时，停止循环。

![](https://youke1.picui.cn/s1/2025/11/12/69144fde56f79.png)

策略迭代的结果就是策略$\pi$和价值函数$V$会收敛到最优$\pi^*$,
$V^*$

价值迭代更为简单粗暴，直接所有的动作，找到令回报最大的价值函数

$$
V(s) \leftarrow \max_{a \in A}(R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V(s'))
$$

在策略选取方面，使用确定性策略，直接选取令回报最大的动作$a$，
即$\pi(s)=arg\max_{a}\sum_{s',r}P(s',r|s,a)[r+\gamma V(s')]$

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch3/figs/vi_pseu.png)

策略迭代与价值迭代的区别：

策略迭代迭代两个目标，价值函数和策略一起收敛，参与动作价值计算的动作$a$随着策略的迭代而变化，即收敛过程是跳变的。而价值迭代要遍历计算所有的动作下的价值，从迭代速度来说，策略迭代更胜一筹。

## 三、蒙特卡洛方法

### 1.蒙特卡洛预测

蒙特卡洛估计方法旨在利用随机采样获得样本的概率分布。

假设求取$f(x)$的期望，即
$$
\mathbb{E}(f(x))=\int f(x)p(x)dx
$$

其中$p(x)$
为随机变量$X$的概率密度函数，实际的概率密度我们不知道，通过采样的方法估计该期望，
假设每一次采样{$x_1,x_2,\cdots,x_N$}都符合独立同分布，根据大数定理，
随着采样次数$N$的增加，估计的期望值会收敛到实际期望值，即
$$
\mathbb{E}(f(x))=\hat{\mathbb{E}}(f(x))=\lim_{N \to \infty}\frac{1}{N}\sum_{i=1}^Nf(x_i)
$$

在强化学习中，用蒙特卡洛方法来估计给定策略$\pi$的状态价值函数
$V_{\pi}$，对每个回合产生的轨迹，对每个状态的回报进行平均，以此估计状态对应的价值，即
$$
V_{\pi}(s)\approx\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_t^i
$$

其中，$N(s)$代表状态
$s$出现的个数，
$G_t^i$代表
第$i$个对应状态
$s$的回报

实际使用增量式更新的方法对状态价值进行估计，边采样边更新，省时省力，更新方式如下：
$$
V(s)\leftarrow V(s)+\frac{1}{N(s)}(G-V(s))=V(s)+\alpha(G-V(s))
$$

$\alpha$为学习率，和一阶离散低通滤波器一模一样，同样取值0到1之间，
$\alpha$越大表示越相信即时回报，$\alpha$越小表示越相信估计值

以增量式更新为基础，又可以分为两种算法：

- 首次访问蒙特卡洛（first-visitMonteCarlo，FVMC）方法：在每个回合的轨迹里，每个状态选择**首次**出现的回报进行更新对应的价值。
- 每次访问蒙特卡洛（every-visitMonteCarlo，EVMC）方法：在每个回合的轨迹里，每个状态选择**所有**出现的回报进行更新对应的价值。

FVMC需要多个回合的数据进行预测，而EVMC计算成本较高。

### 2.蒙特卡洛估计动作价值

和估计价值类似，将价值函数换为动作价值函数————蒙特卡罗方法估计动作价值，计算形式如下：

$$
Q(s,a)\leftarrow Q(s,a)+\alpha(G-Q(s,a))
$$

### 3.蒙特卡洛控制

敲重点理解!!!    
**"价值是从该状态出发的未来回报的期望，而不是仅仅考虑该状态本身的奖励。"**

在动态规划中策略迭代的基础上，采用蒙特卡洛对动作价值进行估计（而非根据状态转移概率，策略概率来计算），然后基于动作价值来改进策略————蒙特卡洛控制。

伪代码如下：

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch4/figs/mc_control_pseu.png)

## 四、时序差分方法
### 1.时序差分估计

蒙特卡洛估计价值，需要计算回报$G$——
从某个状态$s$
一直到回合结束$s^T$的回报，学习进程缓慢。
从而提出了时序差分估计，
$$
V(s_t)\leftarrow V(s_t)+\alpha(R_{t+1}+\gamma V(s_{t+1})-V(s))
$$

该方法只需要当前奖励和下一状态的价值即刻进行估计，其中$R_{t+1}+\gamma V(s_{t+1})$为
$G$的估计值，时序差分误差为：
$$
\delta_t = R_{t+1} + \gamma V(s_{t+1}) - V(s_t)
$$

$R_{t+1}+\gamma V(s_{t+1})$能够对$G$进行估计来源于贝尔曼方程，并且贝尔曼方程是期望形式，需要保证策略$\pi$是稳定的，是收敛的。

使用时序差分方法估计价值，需要多次的迭代更新，以及策略要有一定的探索性（比如使用贪心算法）

要保证价值最后收敛到一定值，其实可以认为每个状态对应的价值其实从频域的角度来说是一种低频信号，在实际应用中，我们可以：
- 设定较小的$\alpha$，用多次迭代去弥补较小的学习率。
- 提高探索率，一开始的误差较大，此举可以减小误差对策略的影响。

利用当前估计来更新现有估计的方法称为————自举
### 2.n步时序差分
回报$G$可以等效为：
$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^{n}V(s_{t+n})
$$

前文的时序差分估计(TD)就是当$n=0$的上式子，随着
$n$的增大，估计值也越趋近于真实回报$G$，在实际应用中，可以适当采取合适的
$n$值来进行训练，较大的
$n$值，会减小开始训练阶段的偏差，但是会有较高的方差。
### 3.Sarsa算法以及Q-learning算法

Sarsa算法以及Q-learning算法的区别在于利用时序差分方法进行更新动作价值的方法不同，即同策略和异策略的方法。

- Sarsa的动作价值更新方法：

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[R_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
$$

其中$a_{t+1}$是采取
$
\epsilon-greedy$算法计算得出的————智能体以概率 
$1 − \epsilon$选择当前估计最优的动作（即具有最高$Q$值的动作），以概率 
$\epsilon$
随机选择一个动作进行探索



- Q-learning的动作价值更新方法：

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[R_{t+1}+\gamma \max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)]
$$

其中$a'$是使得下一个状态Q值最优的一个动作。

Q-learning注重学习最优策略，可能**很快**收敛到最优，但是不稳定（特别是探索不够）。Sarsa考虑实际采取的动作，收敛慢但是
**稳健**。


同策略和异策略的区别：
- 同策略指学习的目标策略和目前使用的策略是同一个策略
- 异策略指学习的目标策略和目前使用的策略不是同一个策略

同策略向自己学习，异策略向最优学习。

## 五、深度学习基础

### 1.从价值表格到价值函数

可以用表格来表示价格函数。比如动作价值函数，动作空间为$A=\{a_1,a_2\}$,
状态空间为$S=\{s_1,s_2\}$,
表示为
$$
\begin{matrix}
& s_1 & s_2 \\
a_1 & 1.0 & 0.0 \\
a_2 & -1.8 & 0.7
\end{matrix}
$$

其中数值就代表对应行列所在的动作和价值空间的Q值，比如$Q(s_1,a_1)=1.0$。

表格表示价值存在一些问题，随着动作空间和状态空间的不断延申，极端一点的情况就是动作空间和状态空间都是连续空间，那么就会有无数种组合，这种情况下，表格是表示不了的，随之而来的解决这个问题的方法就是利用函数去近似表示价值函数。常用的方法有：

- 利用线性函数去近似表示价值函数，利用参数向量$\theta$
将动作-状态对表示的特征向量$\phi(s,a)$
映射到价值，即
$$
Q(s,a)=\theta^T\phi(s,a)
$$

利用线性函数去拟合动作价值，但是我们所面临的绝大部分问题都是非线性的问题，参数调整也较为困难。

- 深度学习出现之后，我们可以利用深度神经网络去拟合动作价值，DNN利用层级的非线性映射可以拟合任意的函数关系，假设深度神经网络模型为$f(x)$，
输入是动作-状态对的特征向量$\phi(s,a)$，那么这个映射关系可以表示为：

$$
Q(s,a)=f(\phi(s,a))
$$

除了线性拟合和DNN之外，还有一些传统的方法去拟合动作价值，决策树、支持向量机（SVM）等等，如下表所示：

![](https://youke1.picui.cn/s1/2025/11/17/691ac5cc2bbf5.png)

### 2.梯度下降与独热编码(one-hot)

#### 2.1 梯度下降

在讨论梯度下降之前，要引入损失函数的概念，损失函数是描述当前网络模型与目标网络模型之间的差距的函数，训练模型的目标就是让这个损失值更小，几乎等于或者趋近于0最好，设价值函数模型为$V_{\theta}(s)$，
实际价值为$V_{\pi}(s)$，损失函数是关于网络模型参数的函数，它的最简单的一种表示方法，均方误差表示为：

$$
L(\theta)=\frac{1}{2}\mathbb{E}[(V_{\pi}(s)-V_{\theta}(s))^2]
$$

其中，加上$\frac{1}{2}$是为了在计算梯度时方便抵消平方项的系数，我们可以通过蒙特卡洛估计价值，利用回报$G_t$作为一个完整回合（即从$s_t$状态一直到
回合结束）的累积回报作为目标值$V_{\pi}(s_t)$，那么损失函数梯度可以表示为

$$
\nabla_{\theta}L(\theta)=\mathbb{E}[(G_t-V_{\theta}(s_t))(-\nabla_{\theta}V_{\theta}(s_t))]
$$

梯度下降，参数更新方法为：

$$
\theta \leftarrow \theta - \alpha(V_{\theta}(s_t)-G_t)\nabla_{\theta}V_{\theta}(s_t)
$$

其中$\alpha为学习率$，可以理解为参数更新的步长。上述参数更新的过程，可以通过SGD、Adam或者Momentum等方法来优化。如果利用时序差分估计，那么目标值可以表示为$V_{\pi}(s_t)=R_{t+1}+\gamma V_{\pi}(s_{t+1})$。

那么$\phi(s,a)$如何得到呢，我们可以通过独热编码，或者特征工程来得到：

独热编码：

对于离散的动作状态空间，在1.的表格里的状态
$\{s_1,s_2\}$可以利用独热编码的形式表示为：

$$
s_1=\begin{bmatrix}1&0 \end{bmatrix}\\
s_2=\begin{bmatrix}0&1 \end{bmatrix}
$$

特征工程：

设状态$\{s_1,s_2\}$的一些特征，比如状态序列（1或者2），有无障碍物（0和1）

$$
\begin{matrix}
& 状态序列 & 有无障碍物 \\
s_1 & 0.33 & 0 \\
s_2 & 0.66 & 1
\end{matrix}
$$

根据特征工程，状态$\{s_1,s_2\}$可以表示为：
$$
s_1=\begin{bmatrix}0.33&0 \end{bmatrix}\\
s_2=\begin{bmatrix}0.66&1 \end{bmatrix}
$$

### 3. 神经网络拓展

神经网络有不同的类型，不同类型的神经网络适合的任务类型不同，总结以下：

![](https://youke1.picui.cn/s1/2025/11/17/691ad02f3489c.png)
