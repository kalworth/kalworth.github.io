# JoyRL组队学习笔记(task06)

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

## 九、DDPG

### 1.DPG

确定性策略梯度（DeterministicPolicyGradient，DPG）的核心就是动作$a$的选择来源于确定性策略
$\mu_{\theta}(s)$，这种策略能够应用于连续动作空间。

$$
a = \mu_{\theta}(s)
$$

目标函数的梯度为

$$
\begin{aligned}
\nabla_{\theta}J(\mu_{\theta}) &= \int_{s_t} p^{\beta} \nabla_{\mu_{\theta}} Q(s_t,\mu_{\theta}(s_t)) \nabla_{\theta} \mu_{\theta}(s_t) ds_t \\
&= \mathbb{E}_{s_t \sim \rho^\beta}[\left.\nabla_a Q\left(s_t, a\right)\right|_{a=\mu_\theta\left(s_t\right)} \nabla_\theta \mu_\theta(s_t)]
\end{aligned}
$$

其中$p^{\beta}$为折扣状态分布，具体见 

https://kalworth.github.io/2025/11/25/Gemini3%E8%BE%93%E5%87%BADPG%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html

### 2. DDPG

DDPG就是在DPG的基础上，利用深度神经网络将Q函数和策略函数参数化，结合Actor-Critic框架，利用经验回放和目标网络进行学习的算法。

为了让动作具备探索性，前文有提到给动作加入高斯噪声，高斯噪声相比OU噪声平滑性偏差，OU噪声$x_t$在动作中体现为

$$
a = \mu_{\theta}(s)+x_t
$$

$x_t$的微分为

$$
d x_t=\theta\left(\mu-x_t\right) d t+\sigma d W_t
$$

其中$\mu$是噪声的均值，一般设置为0，保证探索的无偏性

$\theta$表示回归速率，从感觉上来说就是将噪声拉向$\mu$的速率，从控制的角度来说相当于PID里的
比例$K_p$，理解为噪声
$x_t$跟踪均值
$\mu$的强度。
这样做的好处是什么呢？让噪声去跟踪均值，换句话来说，让噪声回归的均值也符合随着训练的进行，探索程度逐步减小，由于回归的趋势，相比于随机跳变的噪声，OU噪声更加稳定一些。

$\sigma$表示扰动项，是作用于随机噪声
$W_t$的微分的系数，能够控制叠加的噪声的强度，极端一点，如果
$\sigma$太大，可能导致选取的动作是纯纯的噪声，没有策略

DDPG采用软更新来更新目标网络

### 3.TD3

TD3（TwinDelayedDDPG）实在DDPG的基础上，引入了三个改进技巧：

- 双Q网络，避免Critic单个Q网络出现估计值偏高的情况，在计算Critic损失时，引入双Q网络，选择两个Q网络中估计值小的一方作为目标。

- 延迟更新，
>举个例子，
Critic就好比领导，Actor则好比员工，领导不断给员工下达目标，员工不断地去完成目标，如果领导的决策经常失误，那么员工就很容易像无头苍蝇一样不知道该完成哪些目标。
>延迟更新（ 
DelayedPolicyUpdates）是指在 TD3算法中，Actor网络的更新频率要低于 Critic网络的更新频率。

意思就是说，Critic评估的价值作为Actor采取策略的目标，所以要优先保证Critic的正确收敛，目标都是错的，当然学不好策略，就采取Critic更新频率高于Actor频率的方式。

- 目标策略平滑，Critic网络本身就是有误差的，尤其是过拟合的时候，会在某些区域形成“高峰”，为了解决Critic本身在训练过程中存在的误差问题，引入目标策列平滑，在计算目标$y$时候，加入随机噪声如下：

$$
y=r+\gamma Q_{\theta^{\prime}}\left(s^{\prime}, \pi_{\phi^{\prime}}\left(s^{\prime}\right)+\epsilon\right) \epsilon \sim \operatorname{clip}(N(0, \sigma),-c, c)
$$

## 十、PPO