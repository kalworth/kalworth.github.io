# DPG 策略梯度推导过程

**确定性策略梯度（Deterministic Policy Gradient, DPG）** 是强化学习中解决连续动作空间问题的核心理论，也是 DDPG 算法的数学基础。与随机策略不同，确定性策略对于同一个状态 $s$，策略网络 $\mu_\theta(s)$ 直接输出一个确定的动作。

以下是详细的推导步骤。

### 1. 符号与定义

*   **状态**：$s \in \mathcal{S}$
*   **动作**：$a \in \mathcal{A}$
*   **确定性策略**：$a = \mu_\theta(s)$，其中 $\theta$ 是策略参数。
*   **状态价值函数**：$V^\mu(s)$
*   **动作价值函数**：$Q^\mu(s, a)$
*   **初始状态分布**：$p_1(s)$，即 $t=0$ 时刻状态的概率密度。
*   **目标函数** $J(\theta)$：定义为期望的累积折扣回报。

$$
J(\theta) = \int_{\mathcal{S}} p_1(s) V^\mu(s) \, ds
$$

### 2. 推导核心思路

我们需要计算目标函数相对于参数 $\theta$ 的梯度 $\nabla_\theta J(\theta)$。

根据贝尔曼方程，状态价值函数 $V^\mu(s)$ 可以写成 Q 值函数的形式：

$$
V^\mu(s) = Q^\mu(s, \mu_\theta(s))
$$

### 3. 详细推导步骤

#### 第一步：利用链式法则展开 Q 函数的梯度

对于某个状态 $s$，我们对 $V^\mu(s)$ 关于 $\theta$ 求梯度。利用**链式法则**展开：

$$
\nabla_\theta V^\mu(s) = \nabla_\theta Q^\mu(s, \mu_\theta(s)) = \underbrace{\nabla_a Q^\mu(s, a) \big|_{a=\mu_\theta(s)} \nabla_\theta \mu_\theta(s)}_{\text{记为 } g(s)} + \underbrace{\nabla_\theta Q^\mu(s, a) \big|_{a=\mu_\theta(s)}}_{\text{未来 Q 函数变化}}
$$

为了书写简洁，我们令 **$g(s) = \nabla_a Q^\mu(s, a) \big|_{a=\mu_\theta(s)} \nabla_\theta \mu_\theta(s)$**，这代表了当前一步策略调整带来的价值梯度。

#### 第二步：展开 Q 函数的递归定义

根据贝尔曼方程，$Q^\mu(s, a) = r(s, a) + \gamma \int_{\mathcal{S}} p(s' | s, a) V^\mu(s') \, ds'$。
对 $\theta$ 求导（注意 $r$ 和 $p$ 与 $\theta$ 无关）：

$$
\nabla_\theta Q^\mu(s, a) = \gamma \int_{\mathcal{S}} p(s' | s, a) \nabla_\theta V^\mu(s') \, ds'
$$

#### 第三步：代回原方程并递归

将第二步的结果代回第一步的方程中：

$$
\nabla_\theta V^\mu(s) = g(s) + \gamma \int_{\mathcal{S}} p(s' | s, \mu_\theta(s)) \nabla_\theta V^\mu(s') \, ds'
$$

这是一个递归形式。现在我们要计算目标函数的梯度 $\nabla_\theta J(\theta)$，即对初始状态 $s \sim p_1(s)$ 求期望：

$$
\nabla_\theta J(\theta) = \int_{\mathcal{S}} p_1(s) \nabla_\theta V^\mu(s) \, ds
$$

将递归公式代入上式，不断展开 $V^\mu(s')$：

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \int_{\mathcal{S}} p_1(s) \left[ g(s) + \gamma \int_{\mathcal{S}} p(s'|s, \mu) \nabla_\theta V^\mu(s') ds' \right] ds \\
&= \int_{\mathcal{S}} p_1(s) g(s) ds + \gamma \int_{\mathcal{S}} \int_{\mathcal{S}} p_1(s) p(s'|s, \mu) \nabla_\theta V^\mu(s') ds' ds \\
&= \int_{\mathcal{S}} p_1(s) g(s) ds + \gamma \int_{\mathcal{S}} p(s_{t=1}=s'|\mu) \nabla_\theta V^\mu(s') ds'
\end{aligned}
$$

这里的 $p(s_{t=1}=s'|\mu)$ 表示从初始分布出发，经过一步转移后到达状态 $s'$ 的概率密度。如果我们继续无限递归下去，将得到一个无穷级数。

#### 第四步：折扣状态分布 $\rho^\mu$ 的由来 (关键步骤)

经过无限次展开，我们可以将梯度写成所有时间步 $t$ 的累加形式：

$$
\nabla_\theta J(\theta) = \int_{\mathcal{S}} \sum_{t=0}^{\infty} \gamma^t p(s_t = s | \mu) g(s) \, ds
$$

其中 $p(s_t = s | \mu)$ 表示在策略 $\mu$ 下，第 $t$ 个时间步处于状态 $s$ 的概率密度。

我们把积分号里面的求和项定义为 **折扣状态分布 (Discounted State Distribution)** $\rho^\mu(s)$：

$$
\rho^\mu(s) \stackrel{\text{def}}{=} \sum_{t=0}^{\infty} \gamma^t p(s_t = s | \mu)
$$

> **解释**：$\rho^\mu(s)$ 并不是一个标准的归一化概率分布（它的积分和不是1，而是 $1/(1-\gamma)$），它衡量了在考虑折扣因子 $\gamma$ 的情况下，智能体在策略 $\mu$ 下访问状态 $s$ 的“频繁程度”或“重要程度”。

#### 第五步：最终结论

利用定义的 $\rho^\mu(s)$，我们可以把梯度写成紧凑的期望形式：

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \int_{\mathcal{S}} \rho^\mu(s) g(s) \, ds \\
&= \mathbb{E}_{s \sim \rho^\mu} [g(s)] \\
&= \mathbb{E}_{s \sim \rho^\mu} \left[ \nabla_\theta \mu_\theta(s) \cdot \nabla_a Q^\mu(s, a) \big|_{a=\mu_\theta(s)} \right]
\end{aligned}
$$

### 4. 直观理解 (Intuition)

这个公式指导了我们如何更新 Actor（策略网络）：

1.  **$\nabla_\theta \mu_\theta(s)$**：这部分表示“如果我们改变一点点网络参数 $\theta$，输出的动作 $a$ 会怎么变”。
2.  **$\nabla_a Q^\mu(s, a)$**：这部分是 Critic 提供的梯度，表示“如果动作 $a$ 改变一点点，Q 值（预期的好坏）会怎么变”。
3.  **$\rho^\mu$ 的作用**：它告诉我们哪些状态 $s$ 比较重要（经常访问且发生得早），我们应该优先优化这些状态下的策略表现。