# RL笔记
## 一、强化学习基础
### 1.1 强化学习概述

智能体通过获取环境状态$S_t$，采取一个动作$A_t$，环境在这个动作Action(策略 Policy)的影响下进入到下一个状态$S_{t+1}$，并且返回给智能体动作$A_t$所带来的奖励$R_t$。

### 1.2 序列决策

#### 1.2.1 奖励

奖励是环境反馈给智能体的一种标量(scalar)信号，能够衡量在某个状态$S_t$采取某个动作$A_t$下，智能体表现得好坏。强化学习的目的就是最大化奖励，最大化智能体能够在环境中得到的期望累计奖励。

#### 1.2.2 序列决策

在智能体与环境交互的过程中，每一种观测下，智能体都会采取一个动作，然后环境会返回给智能体一个奖励。由观测、动作、奖励组成了整个系统的历史$H_t$：

$$
H_t=o_1,a_1,r_1,...,o_t,a_t,r_t
$$

在一个强化学习环境中，经常存在的问题是奖励具有延迟性，为了最大化期望累计奖励，必须让智能体采取一些具有长期交互来说有利的动作。智能体采取动作需要依赖历史，不能只靠某个时间点的信息来做出决策，那么环境的状态$S_t$被看作是关于历史$H_t$的函数：

$$
S_t=f(H_t)
$$

### 1.3 动作空间

动作空间(action space)：在给定环境中，有效动作的集合。

离散动作空间(discrete action space)：智能体采取的动作只能在可供选择的N(可数)个动作中选择,比如走迷宫的离散动作空间：{向上，向下，向左，向右}。

连续动作空间(continuous action space)：智能体采取的动作可以在一个动作范围中进行选择，可选动作的数量$\infty$，比如旋转倒立摆中的动作空间———电机扭矩：$Torque \in [T_{min},T_{max}]$。

### 1.4 强化学习智能体的组成成分

策略(Policy)：智能体根据当前状态$S_t$，输出一个动作$A_t$。

价值函数(Value Function)：价值函数是对当前状态的评估。智能体根据当前状态$S_t$，输出一个价值$V_t$，价值表示在当前状态下，智能体能够获得的期望累计奖励。

模型(Model)：模型是对环境的一种抽象，能够预测在当前状态$S_t$下，采取动作$A_t$会进入到下一个状态$S_{t+1}$，并且返回给智能体动作$A_t$所带来的奖励$R_{t+1}$。


#### 1.4.1 策略

策略作为一个可以将输入状态映射到输出动作的函数，策略的目标是最大化期望累计奖励。策略可以分为确定性策略(deterministic policy)和随机性策略(stochastic policy)。

确定性策略(deterministic policy)：智能体在当前状态$S_t$下，输出一个确定的动作$A_t$，并且采取这个动作的可能性最大。即

$$
A_t=\arg\max_{a\in A}\pi(a_t|s_t)
$$


随机性策略(stochastic policy)：智能体在当前状态$S_t$下，输出一个动作$A_t$的概率分布。智能体采取的动作根据这个概率分布进行采样。即

$$
A_t\sim\pi(a_t|s_t)
$$

随机性策略相比确定性策略的好处是：
1. 随机性策略可以探索环境中未被尝试过的动作，从而提高智能体的学习效率。
2. 随机性策略可以在动作空间中引入探索性，避免智能体陷入局部最优解。

#### 1.4.2 价值函数

只与状态有关的价值函数，也被称为状态价值函数(state value function)，引入折扣因子$\gamma$，表示未来奖励的折扣率，折扣因子$\gamma$的取值范围为$[0,1]$。状态价值表示为在当前状态$s_t$下，采取策略$\pi$能够获得的期望累计奖励。即


$$
V_\pi(s_t)= E_{\pi}[\sum_{k=0}^\infty\gamma^{k}R_{t+k+1}|S_t=s_t], s_t\in S
$$

其中折扣因子$\gamma^k$与采取策略$\pi$得到的实时奖励$R_{t+k+1}$的乘积，代表了在未来$k$步中，每一步的奖励的折扣值。$\gamma^k$意味着随时间步进行，折扣因子会越来越小，也就是更看重近期奖励。将上式子展开，得到

$$
V_\pi(s_t)\doteq E_{\pi}[\sum_{k=0}^\infty\gamma^{k}R_{t+k+1}|S_t=s_t]=E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|\
S_t=s_t]=E_{\pi}[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...|\
S_t=s_t]=E_{\pi}[R_{t+1}+\gamma V_\pi(s_{t+1})|S_t=s_t]
$$

价值函数的另一种表示方法，Q函数(q-value function)，也被称为动作价值函数(action value function)。Q函数表示在当前状态$s_t$下，采取动作$a_t$能够获得的期望累计奖励。即

$$
Q_\pi(s_t,a_t)\doteq E_{\pi}[\sum_{k=0}^\infty\gamma^{k}R_{t+k+1}|S_t=s_t,A_t=a_t], s_t\in S, a_t\in A(s_t)
$$













