---
title: AgentTuningPaper解析
tags: python,llm,agent
---



开源的LLM在许多领域都有惊人的表现，但是比起商用模型还差很多。这些agent使用LLM作为中心控制器去完成各种任务，需要细粒度的prompt方法和robust LLM来达到令人满意的效果。虽然很多prompt方法已经被提出，但是缺乏一些不损失LLM的通用能力，而关注LLM本身的研究。这篇工作提出AgentTuning，一种简单并且通用的方法来增强LLM的agent能力，同时又保持LLM的通用能力。同时这篇工作提出AgentInstruct，一个轻量级的instruction-tuning数据集。实验以Llama2系列为基座，在AgentInstruct和其他开放数据集上训练，得到的AgentLM-70B与GPT-3.5-turbo在unseen agent任务上能力相当。

总结是，目前LLM在agent上表现一般，因为主要通过prompt + framework 实现

通过三个阶段收集一个交互轨迹：指令构造、使用 GPT-4 作为智能体的轨迹交互以及根据其奖励分数进行轨迹过滤。

给定一个代理任务，LLM代理的交互轨迹可以记录为对话历史（u1，a1，...，un，an）。鉴于现有的对话模型通常包含两个角色：用户和模型，ui 表示来自用户的输入，ai 表示来自模型的响应。每条轨迹都有一个最终奖励 r ∈ [0, 1]，反映任务的完成状态。

**重点在如何构建微调数据上**

Agent turing 数据集构建


给定一个代理任务，LLM代理的交互轨迹可以记录为对话历史（u1，a1，...，un，an）。鉴于现有的对话模型通常包含两个角色：用户和模型，ui 表示来自用户的输入，ai 表示来自模型的响应。每条轨迹都有一个最终奖励 r ∈ [0, 1]，反映任务的完成状态。

主要有三个阶段：

指令构建，轨迹交互，轨迹过滤

if 有训练集，即存在已知的交互轨迹，只需要进行轨迹交互与轨迹过滤（判断轨迹可行性）

else 没有训练集，采用Task Derivation and Self-Instruct构建数据集

Task Derivation

以BIRD 数据库指令为例，有两种获取解决的方案：

1. 从BIRD子任务中获取问题，使用提供的参考SQL语句查询数据库作为 Agent 的答案，要求GPT4生成输出轨迹（这里就指SQL操作）的想法
问题：
交互轮数固定为2，不符合agent 使用的多轮对话背景
2. 与GPT4 交互，让其尝试求解BIRD任务，收集GPT4的求解过程（思考），提供轨迹后将结果与使用参看SQL的结果进行对比，过滤产生错误答案的轨迹

核心就是：**任务一定有解，但解不一定唯一**，通过 GPT4 找出潜在的其他解来扩充数据集

Self-Instruct

对于无法获取/难以获取参考解的问题，使用自指令构造任务
即GPT4 来提供问题与可解思路，另一个GPT4 来尝试在无提示的情况下解决问题并收集轨迹，最终使用一个评估脚本将两个GPT4 的结果对比评估

问题（我自己认为）：高度依赖GPT4生成能力，并且对任务要求很高（一定可解）


完成轨迹获取后，使用GPT4 做轨迹交互评估
Interaction Process


为模型提供任务描述与 1 shot 示例，自行进入循环与环境交互，当agent输出格式错误时使用BLEU指标与可能操作进行比较

CoT Rationales
使用COT来完善交互轨迹的健壮性，对于没有思考的轨迹则使用GPT4 生成来对齐REACT


轨迹过滤

在获取轨迹后尽量保留最终收敛的轨迹（r = 1）
当实验采用 r>2/3 的策略后效果暴降（质量很重要）


INSTRUCTION TUNING

Agent bench 关注的步骤： task -》 交互对象并非人，而是一个一个任务指令（通常是有解而且存在递进条件的）

各种研究表明：对大模型进行指令微调，可使的模型对于新任务具有0-shot的卓越能力（摘要第一句）。但指令微调很大程度上依赖于人类（专家）编写的指令数据，这些些数据在数量、多样性和创造性方面都是有限的，阻碍了调整后的模型的通用性。（摘要第二句）

在训练时使用混合的零样本、少样本和思维链模板（第3.2节）

（II）将 T5 大小的模型扩展到1800+个任务（第3.3节）

（III）通过输入反转来丰富任务（第3.4节），以及（IV）平衡这些任务混合（第3.5节）

如何判断心理治疗的过程是有效的？

**找到了 治疗的靶点**，靶点不充分


心理治疗每一步的目的是什么？

通过交互（什么样） （怎么）-》得到某种心理

如何指导别人进行心理治疗


心理治疗怎么起作用-》心理治疗的医学模型

![Alt text](image-1.png)

心理中的重复性危机

![Alt text](image-2.png)


目前人机对话主要是对单次多轮的情绪安抚研究较多，但心理疾病更复杂，治疗也更有挑战和难度，制定长期多次的个性化对话策略进行治疗是否值得探索和研究？长期多次的治疗有哪些要考虑的因素？

Q* 混合数据微调训练，数据集增大（全参微调）


为什么大模型 COT 能力增强 为什么多智能体框架有效 （上下文管理）


要求过于细时，模型在任务上的平均表现会下降

大模型在持续执行SOP任务时，**无法有效的理解边，但在点的任务上表现良好**

图大模型的主要问题

将图数据和任务转化为语言时可能会丢失图的内部结构，导致模型性能目前尚无法达到预期。

问题 ： 图结构的生成，图结构的理解

图大模型的架构

transformer + LLM

transfomer 受LLM影响更新权重

raph transformer is employed to learn on the graph structure

接着对LLM做微调，微调策略是  Prefix Tuning


SOP2Graph Graph to Task
Task following