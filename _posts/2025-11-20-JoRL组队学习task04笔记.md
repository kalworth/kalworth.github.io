# JoyRL组队学习笔记(task04)

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

## 五、DQN算法

### 1.Q网络

Q-learning算法的更新公式如下：

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [r_t + \gamma \max_{a}Q'(s_{t+1},a) - Q(s_t,a_t)]
$$

其中$y_t = r_t + \gamma \max_{a}Q'(s_{t+1},a)$部分表示目标值或者期望值，是当前网络学习的对象，
设网络模型为$Q_{\theta}(s_t,a_t)$，损失函数可以设定为：

$$
L = (y_t - Q_{\theta}(s_t,a_t))^2
$$

梯度下降更新参数$\theta$的计算方法为：

$$
\theta \leftarrow \theta - \alpha \nabla_{\theta}L(\theta)
$$

### 2.经验回放

在训练网络模型过程中，最耗费时间的是与环境进行交互，而且交互一次获得一个样本去训练模型，很容易导致模型不稳定，而且样本与样本之间在时间上是相互关联的，不符合梯度下降里对样本独立同分布的要求。且与环境交互成本较高，每个样本只被利用一次，数据浪费率较高。

经验回放方法就是将每次与环境交互获得的样本{$s_t,a_t,r_t,s_{t+1}$}存储到经验池子里，每次从经验池中随机抽取一定批次的样本进行训练。交互一次训练一次与采用经验回放进行训练的区别如下图所示：

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch7/figs/replay.png)


### 3.目标网络

如果使用相同网络去得到计算得到目标$y_t$，然后进行梯度下降，这里存在一个问题。

即目标网络和当前网络是同一个网络，梯度下降参数更新之后，目标网络的参数也随之更新，导致训练目标一直在变动，不利于训练的收敛性和稳定性。

目标网络就是复制一个与当前网络结构相同，但是参数不相同的模型$Q_{\theta ^ -}$，目标计算方式为：

$$
y_t=\left\{
\begin{array}
{cc}r_t & \text{对于终止状态}s_t \\
r_t+\gamma\max_{a^{\prime}}Q_{\theta^-}(s_{t+1},a^{\prime}) & \text{对于非终止状态}s_t
\end{array}\right.
$$


并且定期$C$个时间步利用当前网络的参数去更新目标网络的参数

$$
\theta ^ - \leftarrow \theta \ \ Update\ every\ C\  steps
$$

上述方法为**硬更新**方式，
还有一种**软更新**的方式，在DDPG和TD3中被广泛采用

$$
\theta ^ - \leftarrow r \theta + (1-r)\theta ^ -
$$

其中$r$为超参数，表示每次更新靠近当前网络的权重，
当$r$取1的时候，就是硬更新。所以
$r$通常取值很小。

### 4.算法流程

- 初始化网络参数$\theta$
和$\theta ^ -$，以及初始化经验池
$D$

- 根据$\epsilon-greedy$策略采样动作
$a_t$，在状态
$s_t$利用采样动作与环境交互得到
$r_t$和
$s_{t+1}$，将
{$s_t,a_t,r_t,s_{t+1}$}存储到经验池
$D$当中

- 更新状态环境为$s_{t+1}$

- 从经验池$D$中随机抽取一个批量的样本

- 计算期望值$y_t = r_t + \gamma \max_{a} Q_{\theta ^ -}(s_{t+1},a)$

- 计算损失并且梯度下降更新网络参数$\theta$

- 每隔$C$个时间步，利用当前网络参数更新目标网络值$\theta ^ -$

## 六、DQN进阶

### 1. Double DQN

$DQN$在计算目标
$y_t = r_t + \gamma \max_{a}Q_{\theta ^ -}(s_{t+1},a)$上存在一个问题，选择动作和评估价值都是目标网络在做，如果存在目标网络对某一部分动作估值偏高，那么在训练过程中，这一偏差会被逐渐放大——过估计问题。Double DQN提出当前网络取选择动作，利用目标网络评估价值的方法：

$$
y_t = r_t + \gamma Q_{\theta ^ -}(s_{t+1},\max_{a}Q_{\theta}(s_{t+1},a))
$$

上述方法可以减轻过估计问题。

### 2. Dueling DQN

$DQN$无法区分对回报变化影响不大的不同动作，Dueling DQN提出将Q函数分为状态价值
$V(s)$与优势函数
$A(s,a)S$——表示在状态s下选择动作a相比于其他动作的优势。

$$
Q(s,a) = V(s) + A(s,a)
$$

具体的操作就是将Q网络输出层之前分为两个分支，让网络能够同时学习状态价值与动作优势，Q网络更改如图所示

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch8/figs/dqn_network.png)

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch8/figs/dueling_network.png)

但是如果$V(s)=0$的时候，又退化为普通的DQN算法，为了避免这一现象Dueling DQN给优势函数
$A(s,a)S$增加一些约束————去中心化，即

$$
A(s,a) \leftarrow A(s,a) - \frac{1}{\mathcal{A}} \sum_{a' \in A}A(s,a')
$$

这样做让只$更新A(s,a)$参数较为困难，因为其永远是期望为0，这样网络会更倾向于更新
$V(s)$的参数，就可以避免上述问题。

### 3.Noisy DQN

Noisy DQN提出在线性层中添加噪声，让网络本身就有一定探索能力而不是依赖于$\epsilon-greedy$策略。

比如给线性层每一个参数都添加高斯噪声，网络就变为噪声网络

$$
\begin{equation}
Q_{\theta+\epsilon}(s,a)
\end{equation}
$$

Noisy DQN与DQN的区别只是在模型参数中添加了噪声，让模型具备一定探索能力。


### 4. PER DQN

在DQN算法当中，经验回放对所有的样本都是随机采样，可能会忽略一些价值较高，但是出现次数较少的样本。PER DQN提出优先经验回放，利用TD误差对不同重要程度的样本定义一个采样优先级$p_i$

$$
p_i = (|y_{i}-Q(s_i,a_i)|+\epsilon)^\alpha
$$

$\epsilon$为很小的常数，避免TD误差为0导致采样优先级为0，
$\alpha$为控制优先程度的超参数，当
$\alpha=0$时候，每个样本采样率相同，退化为随机采样。

样本被采样的概率为：

$$
P(i) = \frac{p_i }{\sum_k p_k}
$$

给每个样本加上采样概率会影响估计的无偏性，PER DQN引入重要性采样权重解决这个问题

$$
\omega_i = (\frac{1}{N}\cdot\frac{1}{P(i)})^\beta
$$

$\beta$为控制重要性采样程度的超参数，取值(0,1)，通过归一化，采样权重为：

$$
\omega_i \leftarrow \frac{\omega_i}{\max_j \omega_j}
$$

损失函数调整为：

$$
L = \frac{1}{B}\sum_{i=0}^B \omega_i(y_i - Q_{\theta}(s_i,a_i))^2
$$


### 5. 分布式Q函数

Q函数输出的动作价值实质上是价值的期望值，这样就会损失一些信息。分布式Q函数提出直接输出Q值的分布，使用$Z(x,a)$代替原有Q函数，二者的关系为：

$$
Q^\pi(x, a):=\mathbb{E} Z^\pi(x, a)=\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R\left(x_t, a_t\right)\right]
$$