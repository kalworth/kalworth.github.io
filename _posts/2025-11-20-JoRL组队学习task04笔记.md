# JoyRL组队学习笔记(task04)

首先感谢Datawhale组织以及JoyRL教学团队的组队学习以及开源课程！👏👏👏

本次学习活动的课程链接：

https://github.com/datawhalechina/joyrl-book

https://github.com/datawhalechina/easy-rl

## 五、DQN算法

### 1.Q网络

Q-learning算法的更新公式如下：

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [r_t + \gamma \max_{a}Q'(s_{t+1},a) - Q(s_t,a_t)]
$$

其中$y_t = r_t + \gamma \max_{a}Q'(s_{t+1},a)$部分表示目标值或者期望值，是当前网络学习的对象，
设网络模型为$Q_{\theta}(s_t,a_t)$，损失函数可以设定为：

$$
L = (y_t - Q_{\theta}(s_t,a_t))^2
$$

梯度下降更新参数$\theta$的计算方法为：

$$
\theta \leftarrow \theta - \alpha \nabla_{\theta}L(\theta)
$$

### 2.经验回放

在训练网络模型过程中，最耗费时间的是与环境进行交互，而且交互一次获得一个样本去训练模型，很容易导致模型不稳定，而且样本与样本之间在时间上是相互关联的，不符合梯度下降里对样本独立同分布的要求。且与环境交互成本较高，每个样本只被利用一次，数据浪费率较高。

经验回放方法就是将每次与环境交互获得的样本{$s_t,a_t,r_t,s_{t+1}$}存储到经验池子里，每次从经验池中随机抽取一定批次的样本进行训练。交互一次训练一次与采用经验回放进行训练的区别如下图所示：

![](https://datawhalechina.github.io/joyrl-book/rl_basic/ch7/figs/replay.png)


### 3.目标网络

如果使用相同网络去得到计算得到目标$y_t$，然后进行梯度下降，这里存在一个问题。

即目标网络和当前网络是同一个网络，梯度下降参数更新之后，目标网络的参数也随之更新，导致训练目标一直在变动，不利于训练的收敛性和稳定性。

目标网络就是复制一个与当前网络结构相同，但是参数不相同的模型$Q_{\theta ^ -}$，目标计算方式为：

$$
y_t=\left\{
\begin{array}
{cc}r_t & \text{对于终止状态}s_t \\
r_t+\gamma\max_{a^{\prime}}Q_{\theta^-}(s_{t+1},a^{\prime}) & \text{对于非终止状态}s_t
\end{array}\right.
$$


并且定期$C$个时间步利用当前网络的参数去更新目标网络的参数

$$
\theta ^ - \leftarrow \theta \ \ Update\ every\ C\  steps
$$

上述方法为**硬更新**方式，
还有一种**软更新**的方式，在DDPG和TD3中被广泛采用

$$
\theta ^ - \leftarrow r \theta + (1-r)\theta ^ -
$$

其中$r$为超参数，表示每次更新靠近当前网络的权重，
当$r$取1的时候，就是硬更新。所以
$r$通常取值很小。

### 4.算法流程

- 初始化网络参数$\theta$
和$\theta ^ -$，以及初始化经验池
$D$

- 根据$\epsilon-greedy$策略采样动作
$a_t$，在状态
$s_t$利用采样动作与环境交互得到
$r_t$和
$s_{t+1}$，将
{$s_t,a_t,r_t,s_{t+1}$}存储到经验池
$D$当中

- 更新状态环境为$s_{t+1}$

- 从经验池$D$中随机抽取一个批量的样本

- 计算期望值$y_t = r_t + \gamma \max_{a}Q'(s_{t+1},a)$

- 计算损失并且梯度下降更新网络参数$\theta$

- 每隔$C$个时间步，利用当前网络参数更新目标网络值$\theta ^ -$

## 六、DQN进阶

### 1. Double DQN

