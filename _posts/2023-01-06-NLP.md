---
title : 斯坦福CS224N-NLP课程笔记
tags : python
---
# Lecture 1
## 什么是NLP
自然语言处理（NLP）是计算机科学（computer science）、人工智能（artificial intelligence）和语言学（linguistics）这些领域的交叉领域。
它的目的是为了让计算机能够理解语言，来完成对于世界的理解。
## NLP的主要流程
下面的图来自知乎用户codingling  

![image](https://pic3.zhimg.com/80/v2-ad47f792898481de6ed720c949970f26_720w.webp)

输入是语音和文本，对应的第一层就是语音识别和OCR/分词。第二层是形态学，接下来是句法分析和语法分析，最后的是对话分析。cs224n主要关注语音识别、句法分析和语法分析。前两层完成对于信息的提取，第三层实现句意阐述，最后一层实现对于上下文内容的理解
## NLP的难点
NLP任务的难点就在于对于句意的阐释是一个非常困难的问题，往往简单的文字蕴含大量的信息，尤其对于中文而言，在英语语境中，会出现同词不同意，错词不同意的情况  
同样在互联网发展的今天，语言本身的意义泛化也让NLP难度变大，尤其是这种无厘头的泛化，一个词语可能会因为与另一个词语音形相似而发生语义迁移，甚至也有可能发生完全相反的对立语义变化  
总之，对于语义的理解是一个非常艰巨的任务。
## NLP的任务（这部分来自showmeAI的教程部分）
自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇处理。自然语言处理的目标是通过设计算法使得计算机能够“理解”语言，从而能够执行某些特定的任务。不同的任务的难度是不一样的：

* 简单任务
拼写检查 Spell Checking
关键词检索 Keyword Search
同义词查找 Finding Synonyms
* 中级任务
解析来自网站、文档等的信息
* 复杂任务
机器翻译 Machine Translation
语义分析 Semantic Analysis
指代消解 Coreference
问答系统 Question Answering
### 如何表征词汇
在所有的NLP任务中，第一个也是可以说是最重要的共同点是我们如何将单词表示为任何模型的输入。在这里我们不会讨论早期的自然语言处理工作是将单词视为原子符号 atomic symbols。
为了让大多数的自然语言处理任务能有更好的表现，我们首先需要了解单词之间的相似和不同。有了词向量，我们可以很容易地将其编码到向量本身中。
## 词向量
这里重点写写  
### 数学符号
先来了解一下序列模型的数学符号命名规则
给出一句话 **王刚和李明去上班了**
这句话中包含9个字，我们可以将整句话看作一个完整的<1X9>向量，假如说我们的任务是提取其中的人名，那么我们可以设计对应的输出就为一个<1X9>向量，其中为人名组成的为1，不为人名组成的为0。  
也就是这样  
![](https://www.zhihu.com/equation?tex=y%20%3D%20%20%5Cbegin%7Bbmatrix%7D%20%20%201%5C%5C%201%5C%5C%200%5C%5C%201%5C%5C%201%5C%5C%200%5C%5C%200%5C%5C%200%5C%5C%200%20%5Cend%7Bbmatrix%7D)  
需要我们预测的语句，也就是原始的输入如何表示呢，我们希望他仍然是向量化的，而且尽可能的保证原有的信息，这里采用基于词表(例如10000大小的vocabulary)映射，使用one-hot的方式对每个单词进行编码。  
假如我们拥有一个10000大小的词表  
![](https://www.zhihu.com/equation?tex=%5Cleft%5B%20%5Cbegin%7Bmatrix%7D%20%E6%88%91%20%5C%5C%20%E4%BD%A0%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%E7%8E%8B%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%E5%88%9A%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%5Ccdot%20%5C%5C%20%E7%BD%AE%20%5Cend%7Bmatrix%7D%20%5Cright%5D)  
我们可以将输入的每一个部分，都看作是一个1X10000的向量，采用one-hot的思想，出现1的地方也就是该文字在词汇表中的对应位置。  
以**王刚和李明去上班了**这句话举例，这句话在上面的编码思想下就变成了一个<1X9X10000>的一个向量  
将输入表示如下  
![](https://www.zhihu.com/equation?tex=x%20%3D%20%20%5Cbegin%7Bbmatrix%7D%20%20%20x%5E%7B%5Cleft%20%5Clangle%201%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%202%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%203%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%204%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%205%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%206%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%207%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%208%20%5Cright%20%5Crangle%7D%5C%5C%20x%5E%7B%5Cleft%20%5Clangle%209%20%5Cright%20%5Crangle%7D%20%5Cend%7Bbmatrix%7D)  
也就是说这里的![](https://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%20%5Clangle%20t%20%5Cright%20%5Crangle%7D)即为一个1X10000的向量
### 词嵌入