# ç”¨Pythonä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œ

*2023å¹´è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦è®¤çŸ¥è®¡ç®—ä½œä¸š*   
æ½˜ç¬ƒé©¿ 21009201106  

ä¸‹é¢çš„å†…å®¹æ—¨åœ¨å¸®åŠ©ä½ å®Œæˆä»é›¶æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå†…å®¹åŸºç¡€æ¥è‡ªäºvictorzhouçš„ä¸€ç¯‡blogï¼š[Machine Learning for Beginners: An Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/)  
åœ¨ä¸Šæ–‡åŸºç¡€ä¸Šå¯¹å†…å®¹åšäº†ä¼˜åŒ–


## 1. æ„å»ºåŸºæœ¬çš„ç¥ç»å…ƒ

ç¥ç»å…ƒå¾€å¾€æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæœ€åŸºç¡€çš„ç»„æˆéƒ¨åˆ†ï¼Œä¸€ä¸ªç¥ç»å…ƒçš„ä»»åŠ¡æ˜¯ï¼Œå®ƒåº”è¯¥æ¥å—ä¸€ä¸ªè¾“å…¥ï¼Œç„¶åæ ¹æ®å†…éƒ¨çš„æƒé‡æ¥åšä¸€ä¸ªè¾“å‡ºï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°å­¦æ¨¡å‹ï¼Œä¸‹é¢çš„å†…å®¹ä¼šå±•ç¤ºä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»å…ƒï¼Œå®ƒæ¥å—ä¸€ä¸ªäºŒç»´è¾“å…¥ï¼Œç„¶åè¾“å‡ºä¸€ç»´ç»“æœ

![](https://victorzhou.com/a74a19dc0599aae11df7493c718abaf9/perceptron.svg)

<style>
.inline-square {
  margin-left: 5px;
  width: 12px;
  height: 12px;
  display: inline-block;
}
</style>
è¯¥ç¥ç»å…ƒå†…éƒ¨å‘ç”Ÿçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š <span class="inline-square" style="background-color: rgb(200, 0, 0);"></span>  
é¦–å…ˆæ¯ä¸€ç»´çš„ç‰¹å¾éœ€è¦ä¸å¯¹åº”çš„æƒé‡å€¼ç›¸ä¹˜ï¼Œè¿™é‡Œçš„æƒé‡å°±æŒ‡ $w_1$
$$
x_1 \rightarrow x_1 * w_1
$$
$$
x_2 \rightarrow x_2 * w_2
$$

æ¥ä¸‹æ¥å½“æƒé‡è®¡ç®—å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªåç½®å€¼ $bias$ åœ¨è¿™é‡Œå†™ä¸º $b$ : <span class="inline-square" style="background-color: #0f9640;"></span>
$$
(x_1 * w_1) + (x_2 * w_2) + b
$$

æœ€åå¾—åˆ°çš„ç»“æœå°†ç»è¿‡ä¸€ä¸ªæ¿€æ´»å‡½æ•° $f$ ï¼Œåœ¨åé¢çš„æ¼”ç¤ºä¸­ï¼Œæ¿€æ´»å‡½æ•°æˆ‘ä»¬å°†é‡‡ç”¨$sigmod$: <span class="inline-square" style="background-color: rgb(255, 150, 0);"></span>
$$
y = f(x_1 * w_1 + x_2 * w_2 + b)
$$

sigmod å‡½æ•°çš„å›¾åƒå¦‚ä¸‹ï¼Œå®ƒçš„è¾“å‡ºèŒƒå›´ä¸º $(0, 1)$ï¼Œå¯ä»¥å°† $(-\infty, +\infty)$ çš„è¾“å‡ºèŒƒå›´ ç¼©å°åˆ° $(0, 1)$ éå¸¸é€‚åˆå®ŒæˆäºŒåˆ†ç±»ä»»åŠ¡

![](https://victorzhou.com/static/dd5a39500acbef371d8d791d2cd381e0/7e3cb/sigmoid.webp)  



### ä¸€ä¸ªç®€å•ç¤ºä¾‹
å‡è®¾æŒ‰ç…§æˆ‘ä»¬çš„è®¾è®¡å­˜åœ¨è¿™æ ·ä¸€ä¸ªç¥ç»å…ƒï¼Œæƒé‡ä¸º [0, 1] ï¼Œåç½®å€¼ä¸º 4 

$$
w = [0, 1]
$$
$$
b = 4
$$

å½“è¾“å…¥ä¸º $x = [2, 3]$. å®ƒä¼šè¿›è¡Œå¦‚ä¸‹çš„è®¡ç®—ï¼š
$$
\begin{aligned}
(w \cdot x) + b &= ((w_1 * x_1) + (w_2 * x_2)) + b \\
&= 0 * 2 + 1 * 3 + 4 \\
&= 7 \\
\end{aligned}
$$  

$$
y = f(w \cdot x + b) = f(7) = \boxed{0.999}
$$

æœ€åæˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯ $0.999$ ï¼Œè¿™æ ·çš„ä¸€ä¸ªå®Œæ•´çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬ç§°å®ƒä¸ºå‰å‘è®¡ç®—ã€‚


```python
### Pythonä»£ç å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»å…ƒ
import numpy as np

def sigmoid(x):
  # æ¿€æ´»å‡½æ•°çš„è¡¨è¾¾å¼ï¼š f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

class Neuron:
  def __init__(self, weights, bias):
    self.weights = weights
    self.bias = bias

  def feedforward(self, inputs):
    # ä¸æƒé‡ç›¸ä¹˜ï¼Œå†åŠ ä¸Šåç½®å€¼ï¼Œæœ€åç»è¿‡æ¿€æ´»å‡½æ•°
    total = np.dot(self.weights, inputs) + self.bias
    return sigmoid(total)

weights = np.array([0, 1]) # w1 = 0, w2 = 1
bias = 4                   # b = 4
n = Neuron(weights, bias)

x = np.array([2, 3])       # x1 = 2, x2 = 3
print(n.feedforward(x))    # 0.9990889488055994
```

    0.9990889488055994
    

## 2. å°†ç¥ç»å…ƒç»„è£…ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œ

ä¸€ä¸ªå¤æ‚çš„ç¥ç»ç½‘ç»œå¾€å¾€ç”±è®¸å¤šä¸ªç¥ç»å…ƒè¿æ¥ç»„æˆï¼Œè€Œè¿™äº›ç¥ç»å…ƒå¾€å¾€å¯ä»¥çœ‹æˆä¸€å±‚ä¸€å±‚çš„ç»“æ„ï¼Œä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š

![](https://victorzhou.com/77ed172fdef54ca1ffcfb0bba27ba334/network.svg)

è¯¥ç½‘ç»œæœ‰ 2 ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªå¸¦æœ‰ 2 ä¸ªç¥ç»å…ƒçš„éšè—å±‚ ($h_1$ and $h_2$), ä»¥åŠä¸€ä¸ªå…·æœ‰ 1 ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå±‚ ($o_1$). 

>éšè—å±‚æ˜¯è¾“å…¥ï¼ˆç¬¬ä¸€ï¼‰å±‚å’Œè¾“å‡ºï¼ˆæœ€åï¼‰å±‚ä¹‹é—´çš„ä»»ä½•å±‚ã€‚å¯ä»¥æœ‰å¤šä¸ªéšè—å±‚

### ç¥ç»ç½‘ç»œä¸­çš„å‰å‘è®¡ç®—

è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šå›¾æ‰€ç¤ºçš„ç½‘ç»œå¹¶å‡è®¾æ‰€æœ‰ç¥ç»å…ƒå…·æœ‰ç›¸åŒçš„æƒé‡ $w = [0, 1]$ , åç½®å€¼ $b = 0$ , å¹¶ä¸”åŒæ ·ä½¿ç”¨sigmodæ¥ä½œä¸ºæ¿€æ´»å‡½æ•°.

æ¥çœ‹çœ‹è¾“å…¥ $x = [2, 3]$ ä¼šå‘ç”Ÿä»€ä¹ˆ

$$
\begin{aligned}
h_1 = h_2 &= f(w \cdot x + b) \\
&= f((0 * 2) + (1 * 3) + 0) \\
&= f(3) \\
&= 0.9526 \\
\end{aligned}
$$  

$$
\begin{aligned}
o_1 &= f(w \cdot [h_1, h_2] + b) \\
&= f((0 * h_1) + (1 * h_2) + 0) \\
&= f(0.9526) \\
&= \boxed{0.7216} \\
\end{aligned}
$$

æˆ‘ä»¬è¾“å…¥äº† $x = [2, 3]$ å¾—åˆ°äº†ç¥ç»ç½‘ç»œçš„è®¡ç®—å€¼ä¸º $0.7216$.  

ç¥ç»ç½‘ç»œå¯ä»¥æœ‰ä»»æ„æ•°é‡çš„å±‚ï¼Œè¿™äº›å±‚ä¸­å¯ä»¥æœ‰ä»»æ„æ•°é‡çš„ç¥ç»å…ƒã€‚åŸºæœ¬æ€æƒ³ä¿æŒä¸å˜ï¼šé€šè¿‡ç½‘ç»œä¸­çš„ç¥ç»å…ƒå‘å‰é¦ˆé€è¾“å…¥ï¼Œä»¥æœ€ç»ˆè·å¾—è¾“å‡ºã€‚ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»§ç»­ä½¿ç”¨ä¸Šå›¾æ‰€ç¤ºçš„ç½‘ç»œã€‚




### Pythonä»£ç å®ç°å®ç°ç®€å•çš„ç¥ç»ç½‘ç»œ
ç»“æ„å‚è€ƒå¦‚ä¸‹ï¼š  
![](https://victorzhou.com/77ed172fdef54ca1ffcfb0bba27ba334/network.svg)


```python
class OurNeuralNetwork:
  '''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)
  Each neuron has the same weights and bias:
    - w = [0, 1]
    - b = 0
  '''
  def __init__(self):
    weights = np.array([0, 1])
    bias = 0

    # è¿™é‡Œçš„ç¥ç»å…ƒæˆ‘ä»¬ä½¿ç”¨ä¸Šä¸€å°èŠ‚å®šä¹‰çš„ç¥ç»å…ƒ
    self.h1 = Neuron(weights, bias)
    self.h2 = Neuron(weights, bias)
    self.o1 = Neuron(weights, bias)

  def feedforward(self, x):
    out_h1 = self.h1.feedforward(x)
    out_h2 = self.h2.feedforward(x)
    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))

    return out_o1

network = OurNeuralNetwork()
x = np.array([2, 3])
print(network.feedforward(x)) # 0.7216325609518421
```

    0.7216325609518421
    

## 3. è®­ç»ƒæˆ‘ä»¬è®¾è®¡çš„ç¥ç»ç½‘ç»œ, Part 1 Loss

ç°åœ¨æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„æ•°æ®ä¿¡æ¯:

| Name | Weight (lb) | Height (in) | Gender |
| --- | --- | --- | --- |
| Alice | 133 | 65 | F |
| Bob | 160 | 72 | M |
| Charlie | 152 | 70 | M |
| Diana | 120 | 60 | F |  

æˆ‘ä»¬ç°åœ¨å¸Œæœ›èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„ç‰¹å¾ï¼Œæ¯”å¦‚ 180èº«é«˜80å…¬æ–¤çš„ä½“é‡ æ¥å¾—å‡ºä¸€ä¸ªäººçš„æ€§åˆ«  
ç»“æ„å¤§æ¦‚å¦‚ä¸‹å›¾ï¼š
![](https://victorzhou.com/965173626f97e1e6b497a136d0c14ec1/network2.svg)

ä¸ºäº†æ–¹ä¾¿è®­ç»ƒï¼Œæˆ‘ä»¬å°†å¥³æ€§æ€§åˆ«è¡¨ç¤ºä¸º $1$ ï¼Œå°†ç”·æ€§æ€§åˆ«è¡¨ç¤ºä¸º $0$ï¼Œ

| Name | Weight (minus 135) | Height (minus 66) | Gender |
| --- | --- | --- | --- |
| Alice | -2 | -1 | 1 |
| Bob | 25 | 6 | 0 |
| Charlie | 17 | 4 | 0 |
| Diana | -15 | -6 | 1 |

> è¿™é‡Œä¸ä¼šè¿‡å¤šçš„ä»‹ç»ä¸ºä»€ä¹ˆè¿™æ ·å¤„ç†è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”è¿™é‡Œçš„æ•°æ®å¤„ç†éå¸¸ç®€å•ï¼Œå¦‚æœæ‚¨æœ‰æ›´å¤šçš„å…´è¶£ï¼Œå¯ä»¥åœ¨ç½‘ç»œä¸Šè·å–æ›´å¤šå…³äºæ•°æ®é¢„å¤„ç†æ“ä½œçš„ä¿¡æ¯

### æŸå¤±

åœ¨è®­ç»ƒç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ç§æ–¹æ³•æ¥é‡åŒ–å®ƒçš„**å¥½**ç¨‹åº¦ï¼Œä»¥ä¾¿å®ƒå¯ä»¥å°è¯•åšå¾—**æ›´å¥½**ã€‚è¿™å°±æ˜¯æŸå¤±å‡½æ•°ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ï¼š:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_{true} - y_{pred})^2
$$

Let's break this down:

- $n$ æ˜¯æ ·æœ¬æ•°, wä¸º $4$ (Alice, Bob, Charlie, Diana)ã€‚
- $y$ ä»£è¡¨è¢«é¢„æµ‹çš„å˜é‡, ä¹Ÿå°±æ˜¯æ€§åˆ«ï¼ˆ0ï¼Œ1ï¼‰ã€‚
- $y_{true}$ æ˜¯å˜é‡çš„çœŸå®å€¼ï¼ˆâ€œæ­£ç¡®ç­”æ¡ˆâ€ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ $y_{true}$ å½“è¾“å…¥ Alice çš„ç‰¹å¾æ—¶ï¼Œç»“æœåº”è¯¥æ˜¯ $1$ (Female)ã€‚
- $y_{pred}$ æ˜¯å˜é‡çš„é¢„æµ‹å€¼ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„ç½‘ç»œè¾“å‡ºç»“æœã€‚

$(y_{true} - y_{pred})^2$ ç§°ä¸ºå‡æ–¹è¯¯å·®ã€‚æˆ‘ä»¬çš„æŸå¤±å‡½æ•°åªæ˜¯å–æ‰€æœ‰å¹³æ–¹è¯¯å·®çš„å¹³å‡å€¼ï¼ˆå› æ­¤ç§°ä¸ºå‡æ–¹è¯¯å·®ï¼‰ã€‚æˆ‘ä»¬çš„é¢„æµ‹è¶Šå¥½ï¼Œæˆ‘ä»¬çš„æŸå¤±å°±è¶Šä½

é€šå¸¸è¶Šå°çš„losså€¼å°±ä»£è¡¨æ¨¡å‹ç°åœ¨çš„èƒ½åŠ›è¶Šå¼ºï¼ˆä¸è€ƒè™‘è¿‡æ‹Ÿåˆï¼‰

**è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹å°±æ˜¯å¸Œæœ›æœ€å°åŒ–å®ƒçš„æŸå¤±**

### Lossè®¡ç®—çš„ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬çš„ç½‘ç»œæ€»æ˜¯è¾“å‡º0 æ¢å¥è¯è¯´ï¼Œå®ƒç¡®ä¿¡æ‰€æœ‰äººç±»éƒ½æ˜¯ç”·æ€§ğŸ¤”ã€‚æˆ‘ä»¬çš„æŸå¤±æ˜¯ä»€ä¹ˆï¼Ÿ

| Name | $y_{true}$ | $y_{pred}$ | $(y_{true} - y_{pred})^2$ |
| ---- | --------------- | --------------- | ------ |
| Alice | 1 | 0 | 1 |
| Bob | 0 | 0 | 0 |
| Charlie | 0 | 0 | 0 |
| Diana | 1 | 0 | 1 |  

$$
\text{MSE} = \frac{1}{4} (1 + 0 + 0 + 1) = \boxed{0.5}
$$



### Pythonä»£ç å®ç° MSE Loss


```python
import numpy as np

def mse_loss(y_true, y_pred):
  # y_true and y_pred are numpy arrays of the same length.
  return ((y_true - y_pred) ** 2).mean()

y_true = np.array([1, 0, 0, 1])
y_pred = np.array([0, 0, 0, 0])

print(mse_loss(y_true, y_pred)) # 0.5
```

## 4. è®­ç»ƒæˆ‘ä»¬è®¾è®¡çš„ç¥ç»ç½‘ç»œ, Part 2 åå‘ä¼ æ’­, ä¼˜åŒ–ç¥ç»ç½‘ç»œ

æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæ˜ç¡®çš„ç›®æ ‡ï¼šæœ€å°åŒ–ç¥ç»ç½‘ç»œçš„æŸå¤±ã€‚æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬å¯ä»¥æ”¹å˜ç½‘ç»œçš„æƒé‡å’Œåå·®æ¥å½±å“å…¶é¢„æµ‹ï¼Œä½†ç½‘ç»œæƒé‡ä¸åå·®çš„æ”¹å˜ä¸å¯èƒ½ç”±æˆ‘ä»¬éšå¿ƒæ‰€æ¬²åœ°æ¥è¿›è¡Œï¼Œæˆ‘ä»¬å¸Œæœ›æ•´ä¸ªè¿‡ç¨‹æŸå¤±æ˜¯ä¸æ–­å‡å°‘çš„ï¼Œé‚£å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹å‘¢ï¼Ÿ


ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é›†ä¸­åªæœ‰ Aliceï¼š

| Name | Weight (minus 135) | Height (minus 66) | Gender |
| --- | --- | --- | --- |
| Alice | -2 | -1 | 1 |

é‚£ä¹ˆå‡æ–¹è¯¯å·®æŸå¤±å°±æ˜¯ Alice çš„å¹³æ–¹è¯¯å·®ï¼š

$$
\begin{aligned}
\text{MSE} &= \frac{1}{1} \sum_{i=1}^1 (y_{true} - y_{pred})^2 \\
&= (y_{true} - y_{pred})^2 \\
&= (1 - y_{pred})^2 \\
\end{aligned}
$$

è€ƒè™‘æŸå¤±çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½œä¸ºæƒé‡å’Œåå·®çš„å‡½æ•°ã€‚è®©æˆ‘ä»¬æ ‡è®°ç½‘ç»œä¸­çš„æ¯ä¸ªæƒé‡å’Œåå·®ï¼š

![](https://victorzhou.com/27cf280166d7159c0465a58c68f99b39/network3.svg)

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†æŸå¤±å†™æˆå¤šå˜é‡å‡½æ•°ï¼š  

$$
L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)
$$

æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬æƒ³è¦è°ƒæ•´ $w_1$. é‚£å½“æˆ‘ä»¬è°ƒæ•´ $w_1$æ—¶ï¼Œ$L$ ä¼šå¦‚ä½•å˜åŒ–å‘¢? æˆ‘ä»¬å¯ä»¥ç”¨åå¯¼æ•°æ¥è§‚å¯Ÿ$L$çš„å˜åŒ– $\frac{\partial L}{\partial w_1}$ 


åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å°† $\frac{\partial y_{pred}}{\partial w_1}$ é‡å†™ä¸º:

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_{pred}} * \frac{\partial y_{pred}}{\partial w_1}
$$


æˆ‘ä»¬å¯ä»¥è®¡ç®— $\frac{\partial L}{\partial y_{pred}}$ å› ä¸ºæˆ‘ä»¬å·²ç»è®¡ç®—è¿‡ $L = (1 - y_{pred})^2$ :

$$
\frac{\partial L}{\partial y_{pred}} = \frac{\partial (1 - y_{pred})^2}{\partial y_{pred}} = \boxed{-2(1 - y_{pred})}
$$

æ¥ä¸‹æ¥æˆ‘ä»¬ç ”ç©¶å¦‚ä½•è®¡ç®— $\frac{\partial y_{pred}}{\partial w_1}$. æˆ‘ä»¬ç”¨ $h_1, h_2, o_1$ æ¥ä»£è¡¨ç¥ç»ç½‘ç»œæ¯å±‚çš„è¾“å‡º

$$
y_{pred} = o_1 = f(w_5h_1 + w_6h_2 + b_3)
$$
<figcaption>f è¡¨ç¤ºæ¿€æ´»å‡½æ•° sigmoid</figcaption>

å‚æ•° $w_1$ åªå½±å“ $h_1$ (not $h_2$), æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†è®¡ç®—å¼å†™æˆ

$$
\frac{\partial y_{pred}}{\partial w_1} = \frac{\partial y_{pred}}{\partial h_1} * \frac{\partial h_1}{\partial w_1}
$$  

$$
\frac{\partial y_{pred}}{\partial h_1} = \boxed{w_5 * f'(w_5h_1 + w_6h_2 + b_3)}
$$

åŒæ ·çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¯¹ $\frac{\partial h_1}{\partial w_1}$ ä½¿ç”¨é“¾å¼æ³•åˆ™:

$$
h_1 = f(w_1x_1 + w_2x_2 + b_1)
$$  

$$
\frac{\partial h_1}{\partial w_1} = \boxed{x_1 * f'(w_1x_1 + w_2x_2 + b_1)}
$$


$x_1$ ä»£è¡¨ä½“é‡, and $x_2$ ä»£è¡¨èº«é«˜. æ¥ä¸‹æ¥æˆ‘ä»¬æ¥è§‚å¯Ÿsigmodå‡½æ•°çš„å¯¼æ•°

$$
f(x) = \frac{1}{1 + e^{-x}}
$$  

$$
f'(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = f(x) * (1 - f(x))
$$


ç°åœ¨æˆ‘ä»¬å·²ç»å°†æœ€å¼€å§‹çš„ $\frac{\partial L}{\partial w_1}$ æ‹†è§£ä¸ºä¸‹é¢è¿™ä¸ªæˆ‘ä»¬å¯ä»¥è®¡ç®—çš„å¼å­:

$$
\boxed{\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_{pred}} * \frac{\partial y_{pred}}{\partial h_1} * \frac{\partial h_1}{\partial w_1}}
$$

è¿™ç§é€šè¿‡å‘åè®¡ç®—åå¯¼æ•°çš„ç³»ç»Ÿç§°ä¸º**åå‘ä¼ æ’­**

### å¦‚ä½•è®¡ç®—åå¯¼æ•°

æˆ‘ä»¬å°†ç»§ç»­å‡è®¾åªæœ‰ Alice åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­

| Name | Weight (minus 135) | Height (minus 66) | Gender |
| --- | --- | --- | --- |
| Alice | -2 | -1 | 1 |

è®©æˆ‘ä»¬å°†æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸º $1$ æ‰€æœ‰çš„åç½®è®¾ç½®ä¸º  $0$. å½“æˆ‘ä»¬è¿›è¡Œå‰å‘è®¡ç®—ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š

$$
\begin{aligned}
h_1 &= f(w_1x_1 + w_2x_2 + b_1) \\
&= f(-2 + -1 + 0) \\
&= 0.0474 \\
\end{aligned}
$$  

$$
h_2 = f(w_3x_1 + w_4x_2 + b_2) = 0.0474
$$  

$$
\begin{aligned}
o_1 &= f(w_5h_1 + w_6h_2 + b_3) \\
&= f(0.0474 + 0.0474 + 0) \\
&= 0.524 \\
\end{aligned}
$$

è¾“å‡ºçš„ç»“æœä¸º $y_{pred} = 0.524$, è¿™ä¸èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬å‡†ç¡®çš„åˆ†è¾¨å‡ºè¾“å…¥çš„æ˜¯ç”·æ€§ç‰¹å¾($0$)è¿˜æ˜¯å¥³æ€§ç‰¹å¾($1$) ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬è®¡ç®—åå¯¼ $\frac{\partial L}{\partial w_1}$:

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_{pred}} * \frac{\partial y_{pred}}{\partial h_1} * \frac{\partial h_1}{\partial w_1}
$$  

$$
\begin{aligned}
\frac{\partial L}{\partial y_{pred}} &= -2(1 - y_{pred}) \\
&= -2(1 - 0.524) \\
&= -0.952 \\
\end{aligned}
$$  

$$
\begin{aligned}
\frac{\partial y_{pred}}{\partial h_1} &= w_5 * f'(w_5h_1 + w_6h_2 + b_3) \\
&= 1 * f'(0.0474 + 0.0474 + 0) \\
&= f(0.0948) * (1 - f(0.0948)) \\
&= 0.249 \\
\end{aligned}
$$  

$$
\begin{aligned}
\frac{\partial h_1}{\partial w_1} &= x_1 * f'(w_1x_1 + w_2x_2 + b_1) \\
&= -2 * f'(-2 + -1 + 0) \\
&= -2 * f(-3) * (1 - f(-3)) \\
&= -0.0904 \\
\end{aligned}
$$  

$$
\begin{aligned}
\frac{\partial L}{\partial w_1} &= -0.952 * 0.249 * -0.0904 \\
&= \boxed{0.0214} \\
\end{aligned}
$$


è®¡ç®—çš„åå¯¼å¤§äº0ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬ å¢å¤§$w_1$, $L$ ä¹Ÿä¼šå¢å¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å‡å° $w_1$

### éšæœºæ¢¯åº¦ä¸‹é™

**æˆ‘ä»¬ç°åœ¨åŸºæœ¬å·²ç»å®Œæˆäº†è®­ç»ƒçš„æ‰€æœ‰å‡†å¤‡!** æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ”¹å˜æƒé‡å’Œåå·®ä»¥æœ€å°åŒ–æŸå¤±ã€‚åŸºæœ¬ä¸Šå°±æ˜¯è¿™ä¸ªæ›´æ–°æ–¹ç¨‹ï¼š

$$
w_1 \leftarrow w_1 - \eta \frac{\partial L}{\partial w_1}
$$

$\eta$ æ˜¯ä¸€ä¸ªç§°ä¸ºå­¦ä¹ ç‡çš„å¸¸æ•°ï¼Œå®ƒæ§åˆ¶æˆ‘ä»¬è®­ç»ƒçš„é€Ÿåº¦ã€‚æˆ‘ä»¬æ‰€åšçš„åªæ˜¯è®© $w_1$ å‡å» $\eta \frac{\partial L}{\partial w_1}$ :

- å¦‚æœ $\frac{\partial L}{\partial w_1}$ å¤§äº0, $w_1$ å°†ä¼šå‡å°, ä½¿å¾— $L$ ä¸‹é™.
- å¦‚æœ $\frac{\partial L}{\partial w_1}$ å°äº0, $w_1$ å°†ä¼šå¢å¤§, ä½¿å¾— $L$ ä¸‹é™.

å¦‚æœæˆ‘ä»¬å¯¹ç½‘ç»œä¸­çš„æ¯ä¸ªæƒé‡å’Œåç½®éƒ½è¿™æ ·åšï¼ŒæŸå¤±å°±ä¼šæ…¢æ…¢å‡å°‘ï¼Œæˆ‘ä»¬çš„ç½‘ç»œä¹Ÿä¼šå¾—åˆ°æ”¹å–„ã€‚

æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

1. ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·ï¼Œè¿™é‡Œæˆ‘ä»¬åªéšæœºé‡‡å–ä¸€ä¸ªæ ·æœ¬ã€‚
2. è®¡ç®—æŸå¤±ç›¸å¯¹äºæƒé‡æˆ–åå·®çš„æ‰€æœ‰åå¯¼æ•° (e.g. $\frac{\partial L}{\partial w_1}$, $\frac{\partial L}{\partial w_2}$, etc).
3. ä½¿ç”¨SGDæ¥æ›´æ–°æ¯ä¸ªæƒé‡å’Œåå·®
4. é‡å¤æ­¥éª¤1

### å®ç°å®Œæ•´çš„BPç¥ç»ç½‘ç»œ

è®­ç»ƒæ•°æ®å¦‚ä¸‹:

| Name | Weight (minus 135) | Height (minus 66) | Gender |
| --- | --- | --- | --- |
| Alice | -2 | -1 | 1 |
| Bob | 25 | 6 | 0 |
| Charlie | 17 | 4 | 0 |
| Diana | -15 | -6 | 1 |

![](./media-link/neural-network-post/network3.svg)



```python
import numpy as np

def sigmoid(x):
  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

def deriv_sigmoid(x):
  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))
  fx = sigmoid(x)
  return fx * (1 - fx)

def mse_loss(y_true, y_pred):
  # y_true and y_pred are numpy arrays of the same length.
  return ((y_true - y_pred) ** 2).mean()

class OurNeuralNetwork:
  '''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)

  *** DISCLAIMER ***:
  The code below is intended to be simple and educational, NOT optimal.
  Real neural net code looks nothing like this. DO NOT use this code.
  Instead, read/run it to understand how this specific network works.
  '''
  def __init__(self):
    # Weights
    self.w1 = np.random.normal()
    self.w2 = np.random.normal()
    self.w3 = np.random.normal()
    self.w4 = np.random.normal()
    self.w5 = np.random.normal()
    self.w6 = np.random.normal()

    # Biases
    self.b1 = np.random.normal()
    self.b2 = np.random.normal()
    self.b3 = np.random.normal()

  def feedforward(self, x):
    # x is a numpy array with 2 elements.
    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)
    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)
    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)
    return o1

  def train(self, data, all_y_trues):
    '''
    - data is a (n x 2) numpy array, n = # of samples in the dataset.
    - all_y_trues is a numpy array with n elements.
      Elements in all_y_trues correspond to those in data.
    '''
    learn_rate = 0.1
    epochs = 1000 # number of times to loop through the entire dataset

    for epoch in range(epochs):
      for x, y_true in zip(data, all_y_trues):
        # --- Do a feedforward (we'll need these values later)
        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1
        h1 = sigmoid(sum_h1)

        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2
        h2 = sigmoid(sum_h2)

        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3
        o1 = sigmoid(sum_o1)
        y_pred = o1

        # --- Calculate partial derivatives.
        # --- Naming: d_L_d_w1 represents "partial L / partial w1"
        d_L_d_ypred = -2 * (y_true - y_pred)

        # Neuron o1
        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)
        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)
        d_ypred_d_b3 = deriv_sigmoid(sum_o1)

        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)
        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)

        # Neuron h1
        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)
        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)
        d_h1_d_b1 = deriv_sigmoid(sum_h1)

        # Neuron h2
        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)
        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)
        d_h2_d_b2 = deriv_sigmoid(sum_h2)

        # --- Update weights and biases
        # Neuron h1
        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1
        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2
        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1

        # Neuron h2
        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3
        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4
        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2

        # Neuron o1
        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5
        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6
        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3

      # --- Calculate total loss at the end of each epoch
      if epoch % 10 == 0:
        y_preds = np.apply_along_axis(self.feedforward, 1, data)
        loss = mse_loss(all_y_trues, y_preds)
        print("Epoch %d loss: %.3f" % (epoch, loss))

# Define dataset
data = np.array([
  [-2, -1],  # Alice
  [25, 6],   # Bob
  [17, 4],   # Charlie
  [-15, -6], # Diana
])
all_y_trues = np.array([
  1, # Alice
  0, # Bob
  0, # Charlie
  1, # Diana
])

# Train our neural network!
network = OurNeuralNetwork()
network.train(data, all_y_trues)
```

éšç€ç½‘ç»œçš„ä¸æ–­å­¦ä¹ ï¼ŒæŸå¤±ä¸æ–­ä¸‹é™:

![](https://victorzhou.com/static/99e7886af56d6f41b484d17a52f9241b/111e4/loss.webp)  
ä¸‹é¢æˆ‘ä»¬å°±èƒ½çœ‹åˆ°é¢„æµ‹çš„ç»“æœ:


```python
emily = np.array([-7, -3]) # 128 pounds, 63 inches
frank = np.array([20, 2])  # 155 pounds, 68 inches
print("Emily: %.3f" % network.feedforward(emily)) # 0.951 - F
print("Frank: %.3f" % network.feedforward(frank)) # 0.039 - M
```
