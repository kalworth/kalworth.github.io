---
title : 强化学习入坑指北
tags : python
---
# **强化学习入坑速递**
## **第一部分初识概念**
### **专业术语（`Terminologies`）**
* **状态（`state`）**：类似于样本正在处于的空间状态，决定下一步策略的产生
* **动作（`action`）**：样本即将要发生的动作
* **策略（`policy π`）**：根据状态来推断动作的基本标准，输出各动作的概率
* **奖励（`reward`)**：强化学习的目的就是不断优化策略来保证奖励输出最大，奖励作为采取策略的反馈
* **状态转移（`state transition`）**：采取策略后对应的状态发生变化
* **轨迹（`trajectory`）**：训练过程中状态，动作，对应策略输出的动作奖励，下一个状态构成的连续串
### **疑难概念解析**
* **回报（`return`）**：包括未来的奖励总和
$$
U_{t}=R_{t}+R_{t+1}+R_{t+2}+R_{t+3}+\cdots 
$$
* **折扣回报（`discount return`）**：对于奖励而言，目前能够拿到的奖励优于未来的奖励，所以我们设计参数γ对于之后的回报做参数调节
$$
U_{t}=R_{t}+R_{t+1}+R_{t+2}+R_{t+3}+\cdots 
$$
$$ 
{U_{t}}={R_{t}}+{\gamma} R_{t+1}+{\gamma^{2}} R_{t+2}+{\gamma^{3} R_{t+3}}+\cdots 
$$
* **价值函数（`Value functions`）**：由于 ${U_{t}}$ 本身为一个随机变量，我们往往取 ${U_{t}}$ 的期望作为评估的标准
$$
Q_{\pi}\left(s_{t}, a_{t}\right)=\underset{t}{\mathbb{E}}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right]
$$
这里的 $Q_{\pi}$ 我们就称为**动作价值函数**，$Q_{\pi}$ 本身受到 π 的影响，这里又引出一个概念，我们根据无数的policy，求出其中的最大值，使得
$Q_{\pi}$ 最大化，我们称 $Q^{\star}$ 为**最优动作价值函数**
$$
Q^{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right)
$$
**状态价值函数** $V_{\pi}$ :求出**动作价值函数**的期望，可以帮助我们评估目前模型的表现情况 
$$
V_{\pi}\left(s_{t}\right)=\mathbb{E}_{A}\left[Q_{\pi}\left(s_{t}, A\right)\right]
$$
## 价值学习


## 策略学习